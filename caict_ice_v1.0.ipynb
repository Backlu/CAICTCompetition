{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAICT 風機結冰預測 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017/11/02  \n",
    "http://www.industrial-bigdata.com/competition/competitionAction!showDetail.action?competition.competitionId=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fixed Issues **\n",
    "- 抓不到libcusolver.so.8.0\n",
    "    - export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\n",
    "    - export CUDA_HOME=/usr/local/cuda\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import lzma\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateProgress(msg):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def myscore(true_y, pred_y):\n",
    "    n,p =confusion_matrix(true_y, pred_y)\n",
    "    tn,fp,fn,tp =n[0], n[1], p[0], p[1]\n",
    "    score = 1- 0.5*(fp/(tn+fp))- 0.5*(fn/(fn+tp)) \n",
    "    return score, {'tn':tn,'fp':fp,'fn':fn,'tp':tp}\n",
    "\n",
    "save_dir = 'checkpoints/'\n",
    "def get_save_path(net_number):\n",
    "    return save_dir + 'network' + str(net_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rawdata'></a>\n",
    "** Load raw data ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m15 = pd.read_csv('data/ice1/train/15/15_data.csv')\n",
    "oktime_m15 = pd.read_csv('data/ice1/train/15/15_normalInfo.csv')\n",
    "ngtime_m15 = pd.read_csv('data/ice1/train/15/15_failureInfo.csv')\n",
    "\n",
    "data_m21 = pd.read_csv('data/ice1/train/21/21_data.csv')\n",
    "oktime_m21 = pd.read_csv('data/ice1/train/21/21_normalInfo.csv')\n",
    "ngtime_m21 = pd.read_csv('data/ice1/train/21/21_failureInfo.csv')\n",
    "\n",
    "#data_m08 = pd.read_csv('data/ice1/test/08/08_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 根據OK/NG的時間範圍, 對每一筆數據標記\"是否結冰\" ** \n",
    "- 欄位名稱：label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labeling(data, oktime, ngtime):\n",
    "    for index, row in ngtime.iterrows():\n",
    "        data.loc[(data['time']>=row[0]) & (data['time']<=row[1]),'label']=1\n",
    "\n",
    "    for index, row in oktime.iterrows():\n",
    "        data.loc[(data['time']>=row[0]) & (data['time']<=row[1]),'label']=0\n",
    "        \n",
    "preprocess_labeling(data_m15,oktime_m15,ngtime_m15)\n",
    "preprocess_labeling(data_m21,oktime_m21,ngtime_m21)\n",
    "\n",
    "#backup label\n",
    "label_m15 = data_m15['label']\n",
    "label_m21 = data_m21['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 根據NG的時間, 對每一筆數據標記\"事件ID\" **\n",
    "- 欄位名稱：event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_event(data, ngtime):\n",
    "    data['event']=None\n",
    "    for index, row in ngtime.iterrows():\n",
    "        data.loc[(data['time']<=row[1]) & ( pd.isnull(data['event'])) ,'event']=index\n",
    "    \n",
    "preprocess_event(data_m15,ngtime_m15)\n",
    "preprocess_event(data_m15,ngtime_m15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** save read-to-analysis data to pickle **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_m15 = open('data/pickle/data_m15', 'wb')\n",
    "pickle.dump(data_m15, f_m15)\n",
    "\n",
    "f_m21 = open('data/pickle/data_m21', 'wb')\n",
    "pickle.dump(data_m21, f_m21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data resample **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pretrain - label unkonwn data **\n",
    "- unknown data in training set\n",
    "- validataion set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m15['label'] = label_m15\n",
    "data_m21['label'] = label_m21 \n",
    "\n",
    "data_notnull_15 = data_m15[pd.notnull(data_m15['label'])]\n",
    "data_pretrain = data_notnull_15\n",
    "\n",
    "data_null_15 = data_m15[pd.isnull(data_m15['label'])]\n",
    "data_pretest = pd.concat([data_null_15,data_m21])\n",
    "\n",
    "features = ['power','wind_speed','wind_direction','wind_direction_mean','yaw_position','environment_tmp']\n",
    "pipeline = Pipeline([('forest', RandomForestClassifier())])\n",
    "\n",
    "X = data_pretrain[features]\n",
    "y = data_pretrain['label']\n",
    "model = pipeline.fit(X = X, y = y)\n",
    "\n",
    "prediction = model.predict(data_pretest[features])\n",
    "\n",
    "data_m15.loc[pd.isnull(data_m15['label']),'label'] = prediction[:data_null_15.shape[0]]\n",
    "data_m21['label'] = prediction[data_null_15.shape[0]:]\n",
    "#data_m08['label'] = prediction[data_null_15.shape[0]+data_null_21.shape[0]:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build Prediction Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.concat([data_m15[features],data_m21[features]])\n",
    "train_y = pd.concat([data_m15['label'],data_m21['label']]).values.astype(int)\n",
    "train_y = np.eye(2)[train_y]\n",
    "\n",
    "\n",
    "valid_X = data_m21[features]\n",
    "valid_y = label_m21.values.astype(int)\n",
    "valid_y = np.eye(2)[valid_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['power','wind_speed','wind_direction','wind_direction_mean','yaw_position','environment_tmp']\n",
    "\n",
    "n_inputs = len(features)\n",
    "n_classes = 2 \n",
    "\n",
    "tf.reset_default_graph()\n",
    "X =tf.placeholder(tf.float32, [None, n_inputs], name='X')\n",
    "Y_GT =tf.placeholder(tf.float32, [None, n_classes], name='Y_')\n",
    "\n",
    "W = {\n",
    "    'w1': tf.Variable(tf.random_normal([n_inputs,15], stddev=0.01), name='w1'),\n",
    "    'w2': tf.Variable(tf.random_normal([15,7], stddev=0.01), name='w2'),\n",
    "    'w3': tf.Variable(tf.random_normal([7, n_classes]), name='w3')\n",
    "}\n",
    "\n",
    "B = {\n",
    "    'b1': tf.Variable(tf.random_normal([15]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([7]), name='b2'),\n",
    "    'b3': tf.Variable(tf.random_normal([n_classes]), name='b3'),\n",
    "}\n",
    "\n",
    "\n",
    "with tf.name_scope(\"L1\"):\n",
    "    H1 = tf.matmul(X, W['w1']) + B['b1'] \n",
    "    H1 = tf.nn.relu(H1)\n",
    "\n",
    "with tf.name_scope(\"L2\"):\n",
    "    H2 = tf.matmul(H1, W['w2']) + B['b2'] #(-1, 28) matmul (28, hidden_units) => (-1, hidden_units)\n",
    "    H2 = tf.nn.relu(H2)\n",
    "    \n",
    "with tf.name_scope('L3'):\n",
    "    pred = tf.matmul(H2, W['w3']) + B['b3']\n",
    "    pred_sfmx = tf.nn.softmax(pred, name=\"pred_sfmx\")\n",
    "\n",
    "with tf.name_scope('loss_and_acc'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred_sfmx, 1), tf.argmax(Y_GT, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_GT))\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network: 0\n",
      "epoch:0, batch:233, train loss:0.105 acc:0.965, valid loss:0.039 acc:0.988\n",
      "epoch:1, batch:233, train loss:0.106 acc:0.963, valid loss:0.040 acc:0.988\n",
      "epoch:2, batch:233, train loss:0.128 acc:0.958, valid loss:0.040 acc:0.989\n",
      "epoch:3, batch:233, train loss:0.134 acc:0.953, valid loss:0.038 acc:0.989\n",
      "epoch:4, batch:233, train loss:0.124 acc:0.960, valid loss:0.039 acc:0.989\n",
      "epoch:5, batch:233, train loss:0.129 acc:0.952, valid loss:0.039 acc:0.989\n",
      "epoch:6, batch:233, train loss:0.117 acc:0.955, valid loss:0.040 acc:0.989\n",
      "epoch:7, batch:233, train loss:0.090 acc:0.967, valid loss:0.038 acc:0.989\n",
      "epoch:8, batch:233, train loss:0.118 acc:0.960, valid loss:0.039 acc:0.988\n",
      "epoch:9, batch:233, train loss:0.118 acc:0.959, valid loss:0.040 acc:0.989\n",
      "epoch:10, batch:233, train loss:0.118 acc:0.958, valid loss:0.040 acc:0.989\n",
      "epoch:11, batch:233, train loss:0.116 acc:0.961, valid loss:0.040 acc:0.988\n",
      "epoch:12, batch:233, train loss:0.128 acc:0.951, valid loss:0.039 acc:0.989\n",
      "epoch:13, batch:233, train loss:0.122 acc:0.956, valid loss:0.040 acc:0.988\n",
      "epoch:14, batch:233, train loss:0.113 acc:0.953, valid loss:0.039 acc:0.988\n",
      "epoch:15, batch:233, train loss:0.109 acc:0.961, valid loss:0.038 acc:0.988\n",
      "epoch:16, batch:233, train loss:0.116 acc:0.962, valid loss:0.040 acc:0.989\n",
      "epoch:17, batch:233, train loss:0.121 acc:0.961, valid loss:0.040 acc:0.988\n",
      "epoch:18, batch:233, train loss:0.116 acc:0.958, valid loss:0.039 acc:0.988\n",
      "epoch:19, batch:233, train loss:0.095 acc:0.966, valid loss:0.039 acc:0.989\n",
      "epoch:20, batch:233, train loss:0.117 acc:0.961, valid loss:0.038 acc:0.989\n",
      "\n",
      "Neural network: 1\n",
      "epoch:0, batch:233, train loss:0.119 acc:0.959, valid loss:0.042 acc:0.988\n",
      "epoch:1, batch:233, train loss:0.153 acc:0.945, valid loss:0.040 acc:0.987\n",
      "epoch:2, batch:233, train loss:0.103 acc:0.965, valid loss:0.039 acc:0.988\n",
      "epoch:3, batch:233, train loss:0.126 acc:0.955, valid loss:0.039 acc:0.988\n",
      "epoch:4, batch:233, train loss:0.108 acc:0.963, valid loss:0.037 acc:0.990\n",
      "epoch:5, batch:233, train loss:0.113 acc:0.959, valid loss:0.038 acc:0.989\n",
      "epoch:6, batch:233, train loss:0.132 acc:0.951, valid loss:0.034 acc:0.990\n",
      "epoch:7, batch:233, train loss:0.104 acc:0.963, valid loss:0.035 acc:0.989\n",
      "epoch:8, batch:233, train loss:0.109 acc:0.966, valid loss:0.034 acc:0.989\n",
      "epoch:9, batch:233, train loss:0.098 acc:0.967, valid loss:0.035 acc:0.988\n",
      "epoch:10, batch:233, train loss:0.097 acc:0.968, valid loss:0.035 acc:0.988\n",
      "epoch:11, batch:233, train loss:0.112 acc:0.962, valid loss:0.036 acc:0.988\n",
      "epoch:12, batch:233, train loss:0.085 acc:0.971, valid loss:0.033 acc:0.989\n",
      "epoch:13, batch:233, train loss:0.101 acc:0.967, valid loss:0.032 acc:0.990\n",
      "epoch:14, batch:233, train loss:0.102 acc:0.969, valid loss:0.032 acc:0.989\n",
      "epoch:15, batch:233, train loss:0.095 acc:0.967, valid loss:0.038 acc:0.986\n",
      "epoch:16, batch:233, train loss:0.099 acc:0.967, valid loss:0.035 acc:0.988\n",
      "epoch:17, batch:233, train loss:0.098 acc:0.969, valid loss:0.032 acc:0.991\n",
      "epoch:18, batch:233, train loss:0.095 acc:0.969, valid loss:0.030 acc:0.991\n",
      "epoch:19, batch:233, train loss:0.090 acc:0.971, valid loss:0.029 acc:0.991\n",
      "epoch:20, batch:233, train loss:0.105 acc:0.972, valid loss:0.026 acc:0.992\n",
      "\n",
      "Neural network: 2\n",
      "epoch:0, batch:233, train loss:0.144 acc:0.947, valid loss:0.038 acc:0.988\n",
      "epoch:1, batch:233, train loss:0.114 acc:0.959, valid loss:0.034 acc:0.987\n",
      "epoch:2, batch:233, train loss:0.088 acc:0.969, valid loss:0.033 acc:0.988\n",
      "epoch:3, batch:233, train loss:0.108 acc:0.963, valid loss:0.034 acc:0.988\n",
      "epoch:4, batch:233, train loss:0.074 acc:0.978, valid loss:0.031 acc:0.991\n",
      "epoch:5, batch:233, train loss:0.097 acc:0.965, valid loss:0.030 acc:0.990\n",
      "epoch:6, batch:233, train loss:0.089 acc:0.969, valid loss:0.030 acc:0.990\n",
      "epoch:7, batch:233, train loss:0.088 acc:0.969, valid loss:0.029 acc:0.991\n",
      "epoch:8, batch:233, train loss:0.102 acc:0.968, valid loss:0.028 acc:0.992\n",
      "epoch:9, batch:233, train loss:0.103 acc:0.966, valid loss:0.021 acc:0.993\n",
      "epoch:10, batch:233, train loss:0.096 acc:0.965, valid loss:0.019 acc:0.993\n",
      "epoch:11, batch:233, train loss:0.076 acc:0.974, valid loss:0.018 acc:0.994\n",
      "epoch:12, batch:233, train loss:0.079 acc:0.965, valid loss:0.018 acc:0.993\n",
      "epoch:13, batch:233, train loss:0.073 acc:0.976, valid loss:0.019 acc:0.994\n",
      "epoch:14, batch:233, train loss:0.078 acc:0.972, valid loss:0.017 acc:0.994\n",
      "epoch:15, batch:233, train loss:0.088 acc:0.970, valid loss:0.019 acc:0.993\n",
      "epoch:16, batch:233, train loss:0.085 acc:0.965, valid loss:0.018 acc:0.994\n",
      "epoch:17, batch:233, train loss:0.075 acc:0.978, valid loss:0.018 acc:0.993\n",
      "epoch:18, batch:233, train loss:0.088 acc:0.963, valid loss:0.018 acc:0.993\n",
      "epoch:19, batch:233, train loss:0.081 acc:0.969, valid loss:0.018 acc:0.994\n",
      "epoch:20, batch:233, train loss:0.085 acc:0.972, valid loss:0.017 acc:0.994\n",
      "\n",
      "Neural network: 3\n",
      "epoch:0, batch:233, train loss:0.218 acc:0.945, valid loss:0.105 acc:0.990\n",
      "epoch:1, batch:233, train loss:0.182 acc:0.956, valid loss:0.085 acc:0.990\n",
      "epoch:2, batch:233, train loss:0.212 acc:0.945, valid loss:0.083 acc:0.990\n",
      "epoch:3, batch:233, train loss:0.222 acc:0.942, valid loss:0.083 acc:0.990\n",
      "epoch:4, batch:233, train loss:0.212 acc:0.945, valid loss:0.083 acc:0.990\n",
      "epoch:5, batch:233, train loss:0.231 acc:0.939, valid loss:0.083 acc:0.990\n",
      "epoch:6, batch:233, train loss:0.229 acc:0.939, valid loss:0.083 acc:0.990\n",
      "epoch:7, batch:233, train loss:0.201 acc:0.949, valid loss:0.084 acc:0.990\n",
      "epoch:8, batch:233, train loss:0.210 acc:0.946, valid loss:0.082 acc:0.990\n",
      "epoch:9, batch:233, train loss:0.229 acc:0.939, valid loss:0.082 acc:0.990\n",
      "epoch:10, batch:233, train loss:0.178 acc:0.957, valid loss:0.083 acc:0.990\n",
      "epoch:11, batch:233, train loss:0.212 acc:0.945, valid loss:0.083 acc:0.990\n",
      "epoch:12, batch:233, train loss:0.187 acc:0.954, valid loss:0.083 acc:0.990\n",
      "epoch:13, batch:233, train loss:0.256 acc:0.930, valid loss:0.082 acc:0.990\n",
      "epoch:14, batch:233, train loss:0.179 acc:0.957, valid loss:0.081 acc:0.990\n",
      "epoch:15, batch:233, train loss:0.208 acc:0.947, valid loss:0.083 acc:0.990\n",
      "epoch:16, batch:233, train loss:0.224 acc:0.941, valid loss:0.080 acc:0.990\n",
      "epoch:17, batch:233, train loss:0.235 acc:0.937, valid loss:0.081 acc:0.990\n",
      "epoch:18, batch:233, train loss:0.172 acc:0.959, valid loss:0.083 acc:0.990\n",
      "epoch:19, batch:233, train loss:0.224 acc:0.941, valid loss:0.082 acc:0.990\n",
      "epoch:20, batch:233, train loss:0.241 acc:0.935, valid loss:0.083 acc:0.990\n",
      "\n",
      "Neural network: 4\n",
      "epoch:0, batch:233, train loss:0.098 acc:0.964, valid loss:0.041 acc:0.987\n",
      "epoch:1, batch:233, train loss:0.119 acc:0.957, valid loss:0.039 acc:0.989\n",
      "epoch:2, batch:233, train loss:0.108 acc:0.963, valid loss:0.037 acc:0.990\n",
      "epoch:3, batch:233, train loss:0.099 acc:0.969, valid loss:0.038 acc:0.990\n",
      "epoch:4, batch:233, train loss:0.112 acc:0.958, valid loss:0.036 acc:0.989\n",
      "epoch:5, batch:233, train loss:0.105 acc:0.961, valid loss:0.034 acc:0.988\n",
      "epoch:6, batch:233, train loss:0.084 acc:0.973, valid loss:0.035 acc:0.989\n",
      "epoch:7, batch:233, train loss:0.096 acc:0.967, valid loss:0.035 acc:0.989\n",
      "epoch:8, batch:233, train loss:0.091 acc:0.965, valid loss:0.032 acc:0.991\n",
      "epoch:9, batch:233, train loss:0.104 acc:0.963, valid loss:0.032 acc:0.991\n",
      "epoch:10, batch:233, train loss:0.098 acc:0.969, valid loss:0.031 acc:0.991\n",
      "epoch:11, batch:233, train loss:0.114 acc:0.957, valid loss:0.033 acc:0.990\n",
      "epoch:12, batch:233, train loss:0.095 acc:0.967, valid loss:0.033 acc:0.990\n",
      "epoch:13, batch:233, train loss:0.107 acc:0.965, valid loss:0.032 acc:0.989\n",
      "epoch:14, batch:233, train loss:0.083 acc:0.971, valid loss:0.034 acc:0.989\n",
      "epoch:15, batch:233, train loss:0.090 acc:0.969, valid loss:0.030 acc:0.991\n",
      "epoch:16, batch:233, train loss:0.085 acc:0.973, valid loss:0.031 acc:0.990\n",
      "epoch:17, batch:233, train loss:0.095 acc:0.966, valid loss:0.033 acc:0.990\n",
      "epoch:18, batch:233, train loss:0.107 acc:0.965, valid loss:0.031 acc:0.990\n",
      "epoch:19, batch:233, train loss:0.078 acc:0.975, valid loss:0.029 acc:0.991\n",
      "epoch:20, batch:233, train loss:0.073 acc:0.975, valid loss:0.034 acc:0.989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def optimize(epoch, train_X, train_y):\n",
    "    batch_size = 1500\n",
    "    total_batch= len(train_X) / batch_size\n",
    "    for ep in range(epoch+1):\n",
    "        for i in range(int(total_batch)+1):\n",
    "            rnd_idx = np.random.choice(train_X.shape[0], batch_size, replace=False)\n",
    "            batch_x = train_X.iloc[rnd_idx]\n",
    "            batch_y = train_y[rnd_idx]\n",
    "            _, acc_v1, loss_v1= sess.run([optimizer, accuracy,loss], feed_dict={X: batch_x, Y_GT:batch_y})\n",
    "\n",
    "        acc_v2, loss_v2= sess.run([accuracy,loss], feed_dict={X: valid_X , Y_GT: valid_y})\n",
    "        updateProgress('epoch:{x0}, batch:{x4}, train loss:{x1:.3f} acc:{x5:.3f}, valid loss:{x3:.3f} acc:{x2:.3f}'.format(x0=ep,x2=round(acc_v2,3),x3=round(loss_v2,3),x4=i,x1=round(loss_v1,3), x5=round(acc_v1,3)))\n",
    "        print()\n",
    "        \n",
    "num_networks = 5\n",
    "epoch = 20\n",
    "for i in range(num_networks):\n",
    "    print(\"Neural network: {0}\".format(i))\n",
    "    rnd_idx = np.random.choice(train_X.shape[0], int(train_X.shape[0]*0.6), replace=False)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    optimize(epoch, train_X.iloc[rnd_idx], train_y[rnd_idx])\n",
    "    saver.save(sess=sess, save_path=get_save_path(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Prediction ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/network0\n",
      "network 0 score 0.6436225441734982, {'tp': 533, 'fp': 817, 'tn': 187849, 'fn': 1295}\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/network1\n",
      "network 1 score 0.7654754279505345, {'tp': 978, 'fp': 766, 'tn': 187900, 'fn': 850}\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/network2\n",
      "network 2 score 0.713460336666181, {'tp': 782, 'fp': 164, 'tn': 188502, 'fn': 1046}\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/network3\n",
      "network 3 score 0.5, {'tp': 0, 'fp': 0, 'tn': 188666, 'fn': 1828}\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/network4\n",
      "network 4 score 0.7543056041680735, {'tp': 941, 'fp': 1162, 'tn': 187504, 'fn': 887}\n"
     ]
    }
   ],
   "source": [
    "def ensemble_predictions():\n",
    "    pred_labels = []\n",
    "    for i in range(num_networks):\n",
    "        saver.restore(sess=sess, save_path=get_save_path(i))\n",
    "        pred_t = pd.DataFrame(pred_sfmx.eval({X: valid_X}))\n",
    "        pred_t_argm = pred_t.apply(np.argmax,axis=1)\n",
    "        pred_labels.append(pred_t_argm)\n",
    "        s, d=myscore(np.argmax(valid_y, axis=1), pred_t_argm)\n",
    "        print('network {i} score {s}, {d}'.format(i=i,s=s,d=d))\n",
    "    return pred_labels\n",
    "\n",
    "pred_labels = ensemble_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble score 0.6792784690465576, {'tp': 658, 'fp': 264, 'tn': 188402, 'fn': 1170}\n"
     ]
    }
   ],
   "source": [
    "ensemble_pred_labels = np.mean(pred_labels, axis=0)\n",
    "ensemble_pred_labels = np.array(list(map(lambda x: int(1) if x>=0.5 else int(0), ensemble_pred_labels)))\n",
    "s, d=myscore(np.argmax(valid_y, axis=1), ensemble_pred_labels)\n",
    "print('ensemble score {s}, {d}'.format(s=s,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "** Preformance **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_networks = 5,  epoch = 20\n",
    "    - Training Set:\n",
    "        - original score: score: ~ 0.71\n",
    "        - data augmentation score: 0.73\n",
    "        - ensamble score: ?\n",
    "    - Valid Set:\n",
    "        - original score: 0.54\n",
    "        - data augmentation score: 0.668\n",
    "        - ensamble score: 0.679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAICT 風機結冰預測 - (DNNAutoencoder)\n",
    "\n",
    "2017/07/21   \n",
    "徐仕杰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "-  [1. Load Data and Module](#load) \n",
    "-  [2. Modeling - Autoencoder](#modeling1) \n",
    "-  [3. Modeling- Classification](#modeling2)  \n",
    "-  [4. Training](#training)\n",
    "-  [5. Assessment](#assess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "## 1. Load Data and Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import lzma\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import csv\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):  \n",
    "    with tf.name_scope('summaries_'+str(name)):  \n",
    "        mean = tf.reduce_mean(var)  \n",
    "        tf.summary.scalar('mean', mean)  \n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))  \n",
    "        tf.summary.scalar('stddev', stddev)  \n",
    "        tf.summary.scalar('max', tf.reduce_max(var))  \n",
    "        tf.summary.scalar('min', tf.reduce_min(var))  \n",
    "        tf.summary.histogram('histogram', var)  \n",
    "        \n",
    "def updateProgress(msg):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def myscore(true_y, pred_y):\n",
    "    n,p =sk.metrics.confusion_matrix(true_y, pred_y)\n",
    "    tn = n[0]\n",
    "    fp = n[1]\n",
    "    fn = p[0]\n",
    "    tp = p[1]\n",
    "    #print('tn:',tn,'fp:',fp,'fn:',fn,'fp:',fp)\n",
    "    score = 1- 0.5*(fp/(tn+fp))- 0.5*(fn/(fn+tp)) \n",
    "    #print('score',score)\n",
    "    return score, {'tn':tn,'fp':fp,'fn':fn,'tp':tp}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_selection(data, isTest=False):\n",
    "    if isTest:\n",
    "        fixed_variable = ['time','group']\n",
    "    else:\n",
    "        fixed_variable = ['label','event','time','group','timestamp']\n",
    "    #selected_variable = ['pitch1_angle','pitch2_angle','pitch3_angle','pitch1_speed','pitch3_speed','pitch2_speed']\n",
    "    selected_variable = ['pitch1_angle','pitch2_angle','pitch3_angle','pitch1_speed','pitch3_speed','pitch2_speed','power','yaw_position','environment_tmp']\n",
    "    variable = selected_variable + fixed_variable \n",
    "    data = data[variable]\n",
    "    return data\n",
    "\n",
    "def load_resample_data(id, remove_unknown=True, split_case=20, split_validation=True, policy=1, varsel=False, label=None):\n",
    "    filename = 'data/resample_data_p{POLICY}_{ID}.csv'.format(ID=id,POLICY=policy)\n",
    "    data_resample = pd.read_csv(filename)\n",
    "    if varsel:\n",
    "        data_resample=variable_selection(data_resample)\n",
    "    \n",
    "    if remove_unknown:\n",
    "        data_resample = data_resample[data_resample['label']!=2]\n",
    "    \n",
    "    if label is not None:\n",
    "        data_resample = data_resample[data_resample['label']==label]\n",
    "    \n",
    "    data_resample_train = data_resample[data_resample['event']<split_case]\n",
    "    data_resample_valid = data_resample[data_resample['event']>=split_case]\n",
    "\n",
    "    train_aX = data_resample.drop(['label','time','group','timestamp','event'], axis=1).values\n",
    "    train_ay = data_resample['label'].values.astype(int)\n",
    "    train_aY = np.eye(2)[train_ay]\n",
    "    \n",
    "    train_X = data_resample_train.drop(['label','time','group','timestamp','event'], axis=1).values\n",
    "    train_y = data_resample_train['label'].values.astype(int)\n",
    "    valid_X = data_resample_valid.drop(['label','time','group','timestamp','event'], axis=1).values\n",
    "    valid_y = data_resample_valid['label'].values.astype(int)\n",
    "    train_Y = np.eye(2)[train_y]\n",
    "    valid_Y = np.eye(2)[valid_y]\n",
    "\n",
    "    train_abnormal = data_resample_train[data_resample_train['label']==1]\n",
    "    train_normal = data_resample_train[data_resample_train['label']==0]\n",
    "    train_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(train_normal), j=len(train_normal)/len(data_resample_train),k=len(train_abnormal),m=len(train_abnormal)/len(data_resample_train), n=len(data_resample_train))\n",
    "    \n",
    "    valid_abnormal = data_resample_valid[data_resample_valid['label']==1]\n",
    "    valid_normal = data_resample_valid[data_resample_valid['label']==0]\n",
    "    valid_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(valid_normal), j=len(valid_normal)/len(data_resample_valid),k=len(valid_abnormal),m=len(valid_abnormal)/len(data_resample_valid), n=len(data_resample_valid))\n",
    "    if split_validation:\n",
    "        return data_resample, train_X, train_Y, train_y, valid_X, valid_Y, valid_y, train_desc, valid_desc\n",
    "    else:\n",
    "        return data_resample, train_aX, train_aY, train_ay\n",
    "\n",
    "def load_test1_data(id,remove_unknown=True, dropna=True, varsel=False):\n",
    "    filename = 'data/goodformat_{ID}.csv'.format(ID=id)\n",
    "    data_raw = pd.read_csv(filename)\n",
    "    if varsel:\n",
    "        data_raw=variable_selection(data_raw)\n",
    "        \n",
    "    if dropna:\n",
    "        data_raw = data_raw.dropna()\n",
    "    else: \n",
    "        data_raw = data_raw.fillna(0)\n",
    "        \n",
    "    data_X = data_raw.drop(['label','time','group','timestamp'], axis=1).values\n",
    "    data_y = data_raw['label'].values.astype(int)\n",
    "    data_Y = np.eye(2)[data_y]\n",
    "    \n",
    "    data_abnormal = data_raw[data_raw['label']==1]\n",
    "    data_normal = data_raw[data_raw['label']==0]\n",
    "    data_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(data_normal), j=len(data_normal)/len(data_raw),k=len(data_abnormal),m=len(data_abnormal)/len(data_raw), n=len(data_raw))\n",
    "    \n",
    "    return data_raw, data_X, data_Y, data_y, data_desc\n",
    "\n",
    "def load_test2_data(varsel=False):\n",
    "    test_data = pd.read_csv('data/ice1/test/08/08_data.csv')\n",
    "    if varsel:\n",
    "        test_data=variable_selection(test_data,isTest=True)\n",
    "    test_timeidx = test_data['time']\n",
    "    tmp = test_data.drop(['time','group'], axis=1)\n",
    "    test_X = tmp.values\n",
    "    return test_data, test_X, test_timeidx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load input data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data for autoencoder \n",
    "policy=0\n",
    "_, train_auto_X, _, train_auto_y= load_resample_data(id=15,policy=policy,split_validation=False,split_case=8,varsel=False,label=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data for classification\n",
    "policy=1\n",
    "train_raw, train_X, train_Y, train_y, valid_X, valid_Y, valid_y, train_desc, valid_desc = load_resample_data(id=15,policy=policy,varsel=False)\n",
    "valid_raw, valid_21_X, valid_21_Y, valid_21_y= load_resample_data(id=21,policy=policy,split_validation=False,split_case=8,varsel=False)\n",
    "test_raw, test_X,test_timeidx = load_test2_data(varsel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15-valid: 正常:35339 (0.61 percent), 結冰:22179 (0.39 percent), Total:57518\n",
      "15-train: 正常:7680 (0.64 percent), 結冰:4315 (0.36 percent), Total:11995\n"
     ]
    }
   ],
   "source": [
    "print('15-valid:', train_desc)\n",
    "print('15-train:',valid_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling1'></a>\n",
    "## 2. Modeling\n",
    "- Encoder\n",
    "  - input: 26\n",
    "  - 26 x 18\n",
    "  - 18 x 10 \n",
    "  - 10 x 2\n",
    "- Decoder\n",
    "  - 2 x 10\n",
    "  - 10 x 18\n",
    "  - 18 x 26\n",
    "- Classification\n",
    "  - 26 x 2\n",
    "  - output 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 定義參數- Autoencoder **    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_inputs: 26\n",
      "n_classes: 2\n",
      "encoded_dim: 7\n"
     ]
    }
   ],
   "source": [
    "n_inputs = len(train_X[1]) # 每一行的维度\n",
    "n_classes = len(train_Y[1])  # RNN最后的输出類別個数\n",
    "encoded_neural = 7\n",
    "print('n_inputs:',n_inputs)\n",
    "print('n_classes:',n_classes)\n",
    "print('encoded_dim:',encoded_neural)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X =tf.placeholder(tf.float32, [None, n_inputs], name=\"X\")\n",
    "        \n",
    "W = {\n",
    "    'enc_wd1': tf.Variable(tf.truncated_normal(shape =[n_inputs,19], stddev=0.01), name=\"enc_wd1\"),\n",
    "    'enc_wd2': tf.Variable(tf.truncated_normal(shape = [19,13], stddev=0.01), name=\"enc_wd2\"),\n",
    "    'enc_wd3': tf.Variable(tf.truncated_normal(shape =[13,encoded_neural], stddev=0.01), name=\"enc_wd3\"),\n",
    "    'dec_wd1': tf.Variable(tf.truncated_normal(shape =[encoded_neural,13], stddev=0.01), name=\"dec_wd1\"),\n",
    "    'dec_wd2': tf.Variable(tf.truncated_normal(shape =[13,19], stddev=0.01), name=\"dec_wd2\"),\n",
    "    'dec_wd3': tf.Variable(tf.truncated_normal(shape =[19,n_inputs], stddev=0.01), name=\"dec_wd3\"),\n",
    "}\n",
    "\n",
    "B = {\n",
    "    'enc_bd1': tf.Variable(tf.truncated_normal(shape =[19]),name=\"enc_bd1\"),\n",
    "    'enc_bd2': tf.Variable(tf.truncated_normal(shape =[13]), name=\"enc_bd2\"),\n",
    "    'enc_bd3': tf.Variable(tf.truncated_normal(shape =[encoded_neural]), name=\"enc_bd2\"),\n",
    "    'dec_bd1': tf.Variable(tf.truncated_normal(shape =[13]), name=\"dec_bd1\"),\n",
    "    'dec_bd2': tf.Variable(tf.truncated_normal(shape =[19]),name=\"dec_bd2\"),\n",
    "    'dec_bd3': tf.Variable(tf.truncated_normal(shape =[n_inputs]), name=\"dec_bd3\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 定義參數- classification **    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_ = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"Y_\")\n",
    "W2 = {\n",
    "    'cla_wd1': tf.Variable(tf.truncated_normal(shape =[encoded_neural,n_classes], stddev=0.01), name=\"cla_wd1\"),\n",
    "}\n",
    "B2 = {\n",
    "    'cla_bd1': tf.Variable(tf.truncated_normal(shape =[n_classes]),name=\"cla_bd1\"),\n",
    "}\n",
    "trainable_vars = [W2['cla_wd1'],B2['cla_bd1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 網路結構 - Autoencoder **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Encoder layer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Encoder_Layer\"):\n",
    "    H1 = tf.matmul(X, W['enc_wd1']) + B['enc_bd1'] \n",
    "    H1 = tf.nn.sigmoid(H1, name=\"H1\")\n",
    "    H2 = tf.matmul(H1, W['enc_wd2']) + B['enc_bd2'] \n",
    "    H2 = tf.nn.sigmoid(H2, name=\"H2\")\n",
    "    encoded = tf.matmul(H2, W['enc_wd3']) + B['enc_bd3'] \n",
    "    encoded = tf.nn.sigmoid(encoded, name=\"encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Decoder layer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Decoder_Layer\"):\n",
    "    H10 = tf.matmul(encoded, W['dec_wd1']) + B['dec_bd1'] \n",
    "    H10 = tf.nn.relu(H10, name=\"H10\")\n",
    "    H11 = tf.matmul(H10, W['dec_wd2']) + B['dec_bd2'] \n",
    "    H11 = tf.nn.relu(H11, name=\"H11\")\n",
    "    decoded = tf.matmul(H11, W['dec_wd3']) + B['dec_bd3'] \n",
    "    #decoded = tf.nn.sigmoid(decoded, name=\"decoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classification Layer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    _pred = tf.matmul(encoded, W2['cla_wd1']) + B2['cla_bd1']\n",
    "    pred = tf.nn.softmax(_pred, name=\"pred\")\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    Y= tf.add(x= tf.matmul(encoded, W2['cla_wd1'], name='e'), y=B2['cla_bd1'], name='Y')\n",
    "    Y_softmax = tf.nn.softmax(Y, name='Y_softmax') #這個是one-hot的格式\n",
    "    Y_softmax_argmax = tf.argmax(Y_softmax, axis=1) #從one-hot的格式找出最大那個\n",
    "    \n",
    "    correct_prediction = tf.equal(Y_softmax_argmax, tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    #acc_scar = tf.summary.scalar('accuracy', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Loss Function & Optimizer - Autoencoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_auto = tf.reduce_mean(-tf.reduce_sum(X*tf.log(decoded), axis=1))\n",
    "#loss_scar = tf.summary.scalar('loss', loss)\n",
    "optimizer_auto = tf.train.RMSPropOptimizer(0.01).minimize(loss_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_entropy = -1. * X * tf.log(decoded) - (1. - X) * tf.log(1. - decoded)\n",
    "#loss_auto = tf.reduce_mean(cross_entropy)\n",
    "#loss_auto = tf.reduce_mean(tf.pow(decoded - X, 2))\n",
    "#loss_auto = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=decoded, labels=X))\n",
    "\n",
    "loss_auto = - tf.reduce_sum(X * tf.log(decoded))\n",
    "optimizer_auto = tf.train.RMSPropOptimizer(0.1).minimize(loss_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#self.cost = tf.sqrt(tf.reduce_mean(tf.square(self.input_data - self.decode)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Loss Function & Optimizer- Classification **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cla = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_, logits=_pred))\n",
    "optimizer_cla = tf.train.RMSPropOptimizer(0.01).minimize(loss_cla,var_list=trainable_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Training'></a>\n",
    "## 3. Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training - Autoencoder **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, batch:2736 loss:nan\n",
      "epoch:1, batch:2736 loss:nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-350ca3611e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mrnd_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_auto_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_auto_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer_auto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_auto\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 15\n",
    "batch_size = 128\n",
    "total_batch= len(train_auto_X) / batch_size\n",
    "for ep in range(epoch+1):\n",
    "    for i in range(int(total_batch)+1):\n",
    "        rnd_idx = np.random.choice(train_auto_X.shape[0], batch_size, replace=False)\n",
    "        batch_x = train_auto_X[rnd_idx]\n",
    "        _, loss_v= sess.run([optimizer_auto, loss_auto], feed_dict={X: batch_x})\n",
    "        \n",
    "    updateProgress('epoch:{x0}, batch:{x4} loss:{x3}'.format(x0=ep,x3=loss_v,x4=i))\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  Training - Classification **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch = 15\n",
    "batch_size = 128\n",
    "total_batch= len(train_X) / batch_size\n",
    "for ep in range(epoch+1):\n",
    "    for i in range(int(total_batch)+1):\n",
    "        rnd_idx = np.random.choice(train_X.shape[0], batch_size, replace=False)\n",
    "        batch_x = train_X[rnd_idx]\n",
    "        batch_y = train_Y[rnd_idx]\n",
    "        _, loss_v, acc_v= sess.run([optimizer_cla, loss_cla, accuracy], feed_dict={X: batch_x, Y_:batch_y})\n",
    "        \n",
    "    loss_v2, acc_v2= sess.run([loss_cla, accuracy], feed_dict={X: valid_X , Y_: valid_Y})\n",
    "    updateProgress('Ep:{x0}, Bch:{x4}, Tr: loss:{x5}, acc:{x6}, Vd: loss:{x3}, acc:{x2}'.format(x0=ep,x1=i,x2=acc_v2,x3=loss_v2,x4=i,x6=acc_v,x5=loss_v))\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predict Training Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_t = accuracy.eval({X: train_X , Y_: train_Y})\n",
    "pred_t = pd.DataFrame(pred.eval({X: train_X}))\n",
    "#pred_t = pred_t.apply(np.argmax,axis=1)\n",
    "pred_failprop = pred_t[1]\n",
    "pred_result = pred_failprop.copy()\n",
    "\n",
    "pred_result[pred_result >threshold] = 1\n",
    "pred_result[pred_result <=threshold] = 0\n",
    "\n",
    "pd.Series(train_y).value_counts()\n",
    "pred_result.value_counts()\n",
    "\n",
    "s,_=myscore(train_y, pred_result)\n",
    "\n",
    "print('Accuracy:',acc_t)\n",
    "print(\"Precision\", precision_score(train_y, pred_result))\n",
    "print(\"Recall\", sk.metrics.recall_score(train_y, pred_result))\n",
    "print('score',s,_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predict  Validataion Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all X and get the accuracy\n",
    "acc_v = accuracy.eval({X: valid_X , Y_: valid_Y})\n",
    "pred_v = pd.DataFrame(pred.eval({X: valid_X}))\n",
    "#pred_v = pred_v.apply(np.argmax,axis=1)\n",
    "pred_failprop = pred_v[1]\n",
    "pred_result = pred_failprop.copy()\n",
    "\n",
    "pred_result[pred_result >threshold] = 1\n",
    "pred_result[pred_result <=threshold] = 0\n",
    "\n",
    "pred_result.value_counts()\n",
    "\n",
    "s,_=myscore(valid_y, pred_result)\n",
    "\n",
    "print('Accuracy:',acc_v)\n",
    "print(\"Precision\", precision_score(valid_y, pred_result))\n",
    "print(\"Recall\", sk.metrics.recall_score(valid_y, pred_result))\n",
    "print('score',s,_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predict another machine - partial data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all X and get the accuracy\n",
    "acc_t = accuracy.eval({X: valid_21_X , Y_: valid_21_Y})\n",
    "pred_t = pd.DataFrame(pred.eval({X: valid_21_X}))\n",
    "#pred_t = pred_t.apply(np.argmax,axis=1)\n",
    "pred_failprop = pred_t[1]\n",
    "pred_result = pred_failprop.copy()\n",
    "\n",
    "pred_result[pred_result >threshold] = 1\n",
    "pred_result[pred_result <=threshold] = 0\n",
    "\n",
    "pred_result.value_counts()\n",
    "\n",
    "s,_=myscore(valid_21_y, pred_result)\n",
    "\n",
    "print('Accuracy:',acc_t)\n",
    "print(\"Precision\", precision_score(valid_21_y, pred_result))\n",
    "print(\"Recall\", sk.metrics.recall_score(valid_21_y, pred_result))\n",
    "print('score',s,_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predict Test Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_T = pd.DataFrame(pred.eval({X: test_X}))\n",
    "#pred_T = pred_T.apply(np.argmax,axis=1)\n",
    "pred_failprop = pred_T[1]\n",
    "pred_T_result = pred_failprop.copy()\n",
    "\n",
    "\n",
    "pred_T_result[pred_T_result >threshold] = 1\n",
    "pred_T_result[pred_T_result <=threshold] = 0\n",
    "\n",
    "pred_T_result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime=0\n",
    "endTIme=0\n",
    "search_start=True\n",
    "search_end=False\n",
    "abnormal_list=[]\n",
    "test_timeidx=test_timeidx.astype(int)\n",
    "for i,v in enumerate(pred_T_result):\n",
    "    if (v==1) & (search_start):\n",
    "        startTime = test_timeidx[i]\n",
    "        search_end=True\n",
    "        search_start=False\n",
    "    if (v==0) & (search_end):\n",
    "        endTIme=test_timeidx[i]\n",
    "        search_start=True\n",
    "        search_end=False\n",
    "        abnormal_list.append((startTime,test_timeidx[i-1]))\n",
    "print(len(abnormal_list))\n",
    "abnormal_list[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ans=True\n",
    "if(output_ans):\n",
    "    with open('test1_08_results.csv','w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['startTime','endTime'])\n",
    "        for row in abnormal_list:\n",
    "            dummy = csv_out.writerow(row)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Borad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = 'tb-log1'\n",
    "input_data = train_X\n",
    "input_y = train_y\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "#把圖片的label存到metadata.tsv\n",
    "metadata_file_path = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "with open(metadata_file_path, 'w') as metadata_file:\n",
    "    for row in range(len(input_y)):\n",
    "        c=input_y[row]\n",
    "        metadata_file.write('{}\\n'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save variable\n",
    "Input_Var = tf.Variable(input_data, name='Input_var')\n",
    "H1_Var = tf.Variable(H1.eval({X: input_data}), name='H1_var')\n",
    "H2_Var = tf.Variable(H2.eval({X: input_data}), name='H2_Var')\n",
    "H10_Var = tf.Variable(H10.eval({X: input_data}), name='H10_var')\n",
    "H11_Var = tf.Variable(H11.eval({X: input_data}), name='H11_Var')\n",
    "encoded_Var = tf.Variable(encoded.eval({X: input_data}), name='encoded_var')\n",
    "decoded_Var = tf.Variable(decoded.eval({X: input_data}), name='decoded_Var')\n",
    "\n",
    "saver = tf.train.Saver([Input_Var,encoded_Var,decoded_Var,H1_Var,H2_Var,H10_Var,H11_Var])\n",
    "sess.run(Input_Var.initializer)\n",
    "sess.run(encoded_Var.initializer)\n",
    "sess.run(decoded_Var.initializer)\n",
    "sess.run(H1_Var.initializer)\n",
    "sess.run(H2_Var.initializer)\n",
    "sess.run(H10_Var.initializer)\n",
    "sess.run(H11_Var.initializer)\n",
    "\n",
    "saver.save(sess, os.path.join(LOG_DIR, 'autoencoder.ckpt'))\n",
    "\n",
    "#在config裡面用一個embedding關聯 tensor & its metadata\n",
    "config = projector.ProjectorConfig()\n",
    "# One can add multiple embeddings.\n",
    "\n",
    "embedding1 = config.embeddings.add()\n",
    "embedding1.tensor_name = Input_Var.name\n",
    "embedding1.metadata_path = metadata_file_path\n",
    "\n",
    "embedding2 = config.embeddings.add()\n",
    "embedding2.tensor_name = encoded_Var.name\n",
    "embedding2.metadata_path = metadata_file_path\n",
    "\n",
    "embedding3 = config.embeddings.add()\n",
    "embedding3.tensor_name = decoded_Var.name\n",
    "embedding3.metadata_path = metadata_file_path\n",
    "\n",
    "embedding4 = config.embeddings.add()\n",
    "embedding4.tensor_name = H1_Var.name\n",
    "embedding4.metadata_path = metadata_file_path\n",
    "\n",
    "embedding4 = config.embeddings.add()\n",
    "embedding4.tensor_name = H2_Var.name\n",
    "embedding4.metadata_path = metadata_file_path\n",
    "\n",
    "embedding4 = config.embeddings.add()\n",
    "embedding4.tensor_name = H10_Var.name\n",
    "embedding4.metadata_path = metadata_file_path\n",
    "\n",
    "embedding4 = config.embeddings.add()\n",
    "embedding4.tensor_name = H11_Var.name\n",
    "embedding4.metadata_path = metadata_file_path\n",
    "\n",
    "#embedding.sprite.image_path = os.path.join(LOG_DIR, 'img/mnist_10k_sprite.png')\n",
    "# Specify the width and height of a single thumbnail.\n",
    "#embedding.sprite.single_image_dim.extend([28, 28])\n",
    "\n",
    "\n",
    "\n",
    "# Saves a config file that TensorBoard will read during startup.\n",
    "projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=tb-log0 --port=6006"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

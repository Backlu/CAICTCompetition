{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAICT 風機結冰預測 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017/07/12  \n",
    "http://www.industrial-bigdata.com/competition/competitionAction!showDetail.action?competition.competitionId=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Load Data and Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Python modules:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import lzma\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import shutil\n",
    "import csv\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):  \n",
    "    with tf.name_scope('summaries_'+str(name)):  \n",
    "        mean = tf.reduce_mean(var)  \n",
    "        tf.summary.scalar('mean', mean)  \n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))  \n",
    "        tf.summary.scalar('stddev', stddev)  \n",
    "        tf.summary.scalar('max', tf.reduce_max(var))  \n",
    "        tf.summary.scalar('min', tf.reduce_min(var))  \n",
    "        tf.summary.histogram('histogram', var)  \n",
    "        \n",
    "def updateProgress(msg):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def myscore(true_y, pred_y):\n",
    "    n,p =sk.metrics.confusion_matrix(true_y, pred_y)\n",
    "    tn = n[0]\n",
    "    fp = n[1]\n",
    "    fn = p[0]\n",
    "    tp = p[1]\n",
    "    #print('tn:',tn,'fp:',fp,'fn:',fn,'fp:',fp)\n",
    "    score = 1- 0.5*(fp/(tn+fp))- 0.5*(fn/(fn+tp)) \n",
    "    #print('score',score)\n",
    "    return score, {'tn':tn,'fp':fp,'fn':fn,'tp':tp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_resample_data(id, remove_unknown=True, split_case=20):\n",
    "    data_resample_15 = pd.read_csv('data/resample_data_15.csv')\n",
    "    if remove_unknown:\n",
    "        data_resample_15 = data_resample_15[data_resample_15['label']!=2]\n",
    "    \n",
    "    train_resample_15 = data_resample_15[data_resample_15['event']<split_case]\n",
    "    valid_resample_15 = data_resample_15[data_resample_15['event']>=split_case]\n",
    "    train_X = train_resample_15.drop(['label','time','group','timestamp','event'], axis=1).values\n",
    "    train_y = train_resample_15['label'].values.astype(int)\n",
    "    valid_X = valid_resample_15.drop(['label','time','group','timestamp','event'], axis=1).values\n",
    "    valid_y = valid_resample_15['label'].values.astype(int)\n",
    "    train_Y = np.eye(2)[train_y]\n",
    "    valid_Y = np.eye(2)[valid_y]\n",
    "    \n",
    "    train_abnormal = train_resample_15[train_resample_15['label']==1]\n",
    "    train_normal = train_resample_15[train_resample_15['label']==0]\n",
    "    train_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(train_normal), j=len(train_normal)/len(train_resample_15),k=len(train_abnormal),m=len(train_abnormal)/len(train_resample_15), n=len(train_resample_15))\n",
    "    \n",
    "    valid_abnormal = valid_resample_15[valid_resample_15['label']==1]\n",
    "    valid_normal = valid_resample_15[valid_resample_15['label']==0]\n",
    "    valid_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(valid_normal), j=len(valid_normal)/len(valid_resample_15),k=len(valid_abnormal),m=len(valid_abnormal)/len(valid_resample_15), n=len(valid_resample_15))\n",
    "    \n",
    "    return train_X, train_Y, train_y, valid_X, valid_Y, valid_y, train_desc, valid_desc\n",
    "\n",
    "def load_test1_data(id,remove_unknown=True):\n",
    "    data_raw_21 = pd.read_csv('data/goodformat_21.csv')\n",
    "    data_raw_21 = data_raw_21.dropna()\n",
    "    \n",
    "    data_X = data_raw_21.drop(['label','time','group','timestamp'], axis=1).values\n",
    "    data_y = data_raw_21['label'].values.astype(int)\n",
    "    data_Y = np.eye(2)[data_y]\n",
    "    \n",
    "    data_abnormal = data_raw_21[data_raw_21['label']==1]\n",
    "    data_normal = data_raw_21[data_raw_21['label']==0]\n",
    "    data_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(data_normal), j=len(data_normal)/len(data_raw_21),k=len(data_abnormal),m=len(data_abnormal)/len(data_raw_21), n=len(data_raw_21))\n",
    "    \n",
    "    return data_X, data_Y, data_y, data_desc\n",
    "\n",
    "def load_test2_data():\n",
    "    test_data = pd.read_csv('data/ice1/test/08/08_data.csv')\n",
    "    test_timeidx = test_data['time']\n",
    "    tmp = test_data.drop(['time','group'], axis=1)\n",
    "    test_X = tmp.values\n",
    "    return test_X, test_timeidx\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load input data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, train_y, valid_X, valid_Y, valid_y, train_desc, valid_desc = load_resample_data(15)\n",
    "valid_21_X, valid_21_Y, valid_21_y, valid_21_desc= load_test1_data(21)\n",
    "test_X,test_timeidx = load_test2_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 為了RNN需求, 更改X format **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_dataset(data, n_steps):\n",
    "    dataset_new = []\n",
    "    for idx, _ in enumerate(data):\n",
    "        start_idx = idx-n_steps+1\n",
    "        end_idx = idx+1\n",
    "        if start_idx<0:\n",
    "            continue\n",
    "        n=np.concatenate(data[start_idx:end_idx], axis=0)\n",
    "        dataset_new.append(n)\n",
    "    dataset_new = np.asarray(dataset_new)\n",
    "    return dataset_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100  \n",
    "train_X_new = rnn_dataset(train_X,n_steps)\n",
    "valid_X_new = rnn_dataset(valid_X,n_steps)\n",
    "valid_21_X_new = rnn_dataset(valid_21_X,n_steps)\n",
    "test_X = rnn_dataset(test_X,n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Y_new=train_Y[n_steps:]\n",
    "train_y_new=train_y[n_steps:]\n",
    "valid_Y_new=valid_Y[n_steps:]\n",
    "valid_y_new=valid_y[n_steps:]\n",
    "valid_21_Y_new=valid_21_Y[n_steps:]\n",
    "valid_21_y_new=valid_21_y[n_steps:]\n",
    "test_timeidx_new=test_timeidx[n_steps:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**風機15 Training Data 狀態正常(0)/異常(1)比例 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常:35339 (0.61 percent), 結冰:22179 (0.39 percent), Total:57518\n"
     ]
    }
   ],
   "source": [
    "print(train_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**風機15  Validation Data 狀態正常(0)/異常(1)比例 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常:7680 (0.64 percent), 結冰:4315 (0.36 percent), Total:11995\n"
     ]
    }
   ],
   "source": [
    "print(valid_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 風機21 Testing Data 狀態正常(0)/異常(1)比例 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常:168930 (0.94 percent), 結冰:10638 (0.06 percent), Total:179568\n"
     ]
    }
   ],
   "source": [
    "print(valid_21_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tensorflow 參數定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_inputs = len(train_X[1]) # 每一行的维度\n",
    "n_hidden_unins = 13 # 中間RNN的hidden units\n",
    "n_classes = len(train_Y[1])  # RNN最后的输出類別個数\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X =tf.placeholder(tf.float32, [None, n_steps, n_inputs], name=\"X\")\n",
    "Y_ =tf.placeholder(tf.float32, [None, n_classes], name=\"Y_\")\n",
    "\n",
    "W = {\n",
    "    'in': tf.Variable(tf.random_uniform([n_inputs, n_hidden_unins], -1.0, 1.0), name=\"In_W\"), # 输入层到中间层权重矩阵\n",
    "    'out': tf.Variable(tf.random_uniform([n_hidden_unins, n_classes], -1.0, 1.0), name=\"Out_W\"), # 中间层到输出层的权重\n",
    "}\n",
    "\n",
    "variable_summaries(W['in'],'In_W')\n",
    "variable_summaries(W['out'],'Out_W')\n",
    "B = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_unins]), name=\"In_Bias\"), # 偏置\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes]), name=\"Out_Bias\"),\n",
    "}\n",
    "variable_summaries(B['in'],'In_Bias')\n",
    "variable_summaries(B['out'],'OUT_Bias')\n",
    "\n",
    "#tfdot()  太亂了不看~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 網路結構 Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden_layer for input\n",
    "# X : (128, 28, 28)\n",
    "with tf.name_scope(\"inLayer\"):\n",
    "    X_ = tf.reshape(X, [-1, n_inputs])  # [batch_size*28, 28]\n",
    "    X_in = tf.matmul(X_, W['in']) + B['in'] # (-1, 28) matmul (28, hidden_units) => (-1, hidden_units)\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_unins]) # (batch_size, steps, input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_CELL\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_unins) \n",
    "    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(lstm_cell, X_in, dtype=tf.float32) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('outlayer'):\n",
    "    _pred = tf.matmul(rnn_states[1], W['out']) + B['out']\n",
    "    pred = tf.nn.softmax(_pred, name=\"pred\")\n",
    "    #rnn_outputs = tf.unstack(tf.transpose(rnn_outputs, [1,0,2]))  \n",
    "    #pred = tf.matmul(rnn_outputs[-1], weights['out']) + biases['out'] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 誤差函數 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_pred, labels=Y_))\n",
    "    loss_scar = tf.summary.scalar('loss', loss)\n",
    "    acc_scar = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimizer (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001 # learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 開始訓練  Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先創造一個session, 然後記得要init variable\n",
    "init = tf.global_variables_initializer()\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "#if not os.path.exists('tb-log_1'):\n",
    "#    os.mkdir('tb-log_1')\n",
    "#summary_writer_train = tf.summary.FileWriter('tb-log1/train',graph=tf.get_default_graph())\n",
    "#summary_writer_validation = tf.summary.FileWriter('tb-log1/validation',graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 787800 into shape (300,100,26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-22f00b7ed2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_X_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_Y_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_v1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_v1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_scar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#summary_writer_train.add_summary(summary, ep * total_batch + i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 787800 into shape (300,100,26)"
     ]
    }
   ],
   "source": [
    "epoch = 15\n",
    "batch_size = 300\n",
    "total_batch= len(train_X) / batch_size\n",
    "for ep in range(epoch+1):\n",
    "    for i in range(int(total_batch)+1):\n",
    "        rnd_idx = np.random.choice(train_X_new.shape[0], batch_size, replace=False)\n",
    "        batch_x = train_X_new[rnd_idx]\n",
    "        batch_y = train_Y_new[rnd_idx]\n",
    "        batch_x = batch_x.reshape([batch_size, n_steps, n_inputs])\n",
    "        _, loss_v1, acc_v1= sess.run([optimizer, loss_scar, acc_scar], feed_dict={X: batch_x, Y_:batch_y})\n",
    "        #summary_writer_train.add_summary(summary, ep * total_batch + i)\n",
    "\n",
    "    batch_vx = valid_X_new.reshape([-1, n_steps, n_inputs])\n",
    "    loss_v2, acc_v2= sess.run([loss, accuracy], feed_dict={X: batch_vx , Y_: valid_Y_new})\n",
    "    #summary_writer_validation.add_summary(summary, ep * total_batch + i)\n",
    "    updateProgress('epoch:{x0}, batch:{x4}, train loss:{x1:.3f} acc:{x5:.3f}, test loss:{x3:.3f} acc:{x2:.3f}'.format(x0=ep,x2=round(acc_v2,3),x3=round(loss_v2,3),x4=i,x1=round(loss_v1,3), x5=round(acc_v1,3)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2626)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787800"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2626*300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 預測與準確率評估  Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all X and get the accuracy\n",
    "train_X_rs = train_X.reshape([-1, n_steps, n_inputs])\n",
    "acc_t = accuracy.eval({X: train_X_rs , Y_: test_Y})\n",
    "print('Accuracy:',acc_t)\n",
    "\n",
    "pred_t = pd.DataFrame(pred.eval({X: train_X_rs}))\n",
    "pred_t = pred_t.apply(np.argmax,axis=1)\n",
    "pd.Series(train_y).value_counts()\n",
    "train_pred.value_counts()\n",
    "\n",
    "print(\"Precision\", precision_score(train_y, pred_t))\n",
    "print(\"Recall\", sk.metrics.recall_score(train_y, pred_t))\n",
    "\n",
    "s,_=myscore(train_y, pred_t)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict  Validataion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all X and get the accuracy\n",
    "valid_X_rs = valid_X.reshape([-1, n_steps, n_inputs])\n",
    "acc_v = accuracy.eval({X: valid_X_rs , Y_: valid_Y})\n",
    "print('Accuracy:',acc_v)\n",
    "\n",
    "pred_v = pd.DataFrame(pred.eval({X: valid_X_rs}))\n",
    "pred_v = pred_v.apply(np.argmax,axis=1)\n",
    "pred_v.value_counts()\n",
    "\n",
    "print(\"Precision\", precision_score(valid_y, pred_v))\n",
    "print(\"Recall\", sk.metrics.recall_score(valid_y, pred_v))\n",
    "\n",
    "s,_=myscore(valid_y, pred_v)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predict another machine **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all X and get the accuracy\n",
    "valid_21_X_rs = valid_21_X.reshape([-1, n_steps, n_inputs])\n",
    "acc_t = accuracy.eval({X: valid_21_X_rs , Y_: valid_21_Y})\n",
    "print('Accuracy:',acc_t)\n",
    "pred_t = pd.DataFrame(pred.eval({X: valid_21_X_rs}))\n",
    "pred_t = pred_t.apply(np.argmax,axis=1)\n",
    "pred_t.value_counts()\n",
    "\n",
    "print(\"Precision\", precision_score(valid_21_y, pred_t))\n",
    "print(\"Recall\", sk.metrics.recall_score(valid_21_y, pred_t))\n",
    "\n",
    "s,_=myscore(valid_21_y, pred_t)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_rs = test_X.reshape([-1, n_steps, n_inputs])\n",
    "pred_T = pd.DataFrame(pred.eval({X: test_X}))\n",
    "pred_T = pred_T.apply(np.argmax,axis=1)\n",
    "pred_T.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 整理預測結果, 正確上傳格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime=0\n",
    "endTIme=0\n",
    "search_start=True\n",
    "search_end=False\n",
    "abnormal_list=[]\n",
    "for i,v in enumerate(pred_T):\n",
    "    if (v==1) & (search_start):\n",
    "        startTime = test_timeidx[i]\n",
    "        search_end=True\n",
    "        search_start=False\n",
    "    if (v==0) & (search_end):\n",
    "        endTIme=test_timeidx[i]\n",
    "        search_start=True\n",
    "        search_end=False\n",
    "        abnormal_list.append((startTime,test_timeidx[i-1]))\n",
    "len(abnormal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_ans=False\n",
    "if(output_ans):\n",
    "    with open('test1_08_results.csv','w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['startTime','endTime'])\n",
    "        for row in abnormal_list:\n",
    "            dummy = csv_out.writerow(row)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Issue\n",
    "1. Data Imbalance\n",
    " - training batch sample 一半normal一半abnormal的data\n",
    " - 用兩台風機訓練兩個model, 取故障預測的聯集 \n",
    "\n",
    "2. 用Precision & Recall評估好壞, 算出Score\n",
    "\n",
    "\n",
    "other:\n",
    "- using weighted examples. Just amplify the per-instance loss by a larger weight when seeing positive examples. If you use online gradient descent, it can be as simple as using a larger learning rate when seeing positive examples.\n",
    "- A similar and slightly better approach (only if you use stochastic gradient descent) is randomly picking an example in each iteration, where the positive examples have higher probability of being picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "提交结果历史记录  \n",
    "参赛队伍:　556 / 参赛人数:　858  \n",
    " \n",
    "|竞赛阶段\t |上传者\t |分数\t |提交日期\t |排名\t |下载|\n",
    "| ---------| -------- | ------ | ------ | ------ | ------ |\n",
    "|初赛test1阶段|\t孔祥千\t|55.28684214|\t2017/7/12|\t50\t|下载|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 7/13\n",
    "1. cross correlation\n",
    "2. auto correlation\n",
    "3. common filter\n",
    "4. de-train "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

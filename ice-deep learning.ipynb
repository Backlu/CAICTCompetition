{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAICT 風機結冰預測 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017/07/12  \n",
    "http://www.industrial-bigdata.com/competition/competitionAction!showDetail.action?competition.competitionId=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 題目簡介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 赛程安排\n",
    "本次大赛分为初赛和复赛两个阶段，其中：初赛由参赛队伍下载数据在本地进行算法设计和调试；复赛要求参赛者进行现场答辩。具体安排和要求如下：  \n",
    "\n",
    "#### 初赛（2017年6月15日—9月1日）  \n",
    "- 1、6月15日打开报名通道，9月1日关闭报名通道，报名规则详见组队规则。  \n",
    "- 2、参赛队伍$\\color{blue}{7月3日可下载数据}$，并在本地调试算法，提交结果。  \n",
    "- 3、7月3日起提供每天1 次的评测和排名机会，选手上传数据即可获得评分与排名，实时更新排行榜，按照评测指标从高到低排序；(排行榜将选择选手在本阶段的历史最优成绩进行排名展示，不做最终排名计算)。  \n",
    "- 4、$\\color{blue}{8月18日组委会将提供最终数据}$，所有参赛者对这组数据仅有一次提交机会，且选手除提交数据外，还需上传技术原理及相关源代码，参赛者的初赛最终成绩将由这组最终数据的预测结果决定。 注：不提供技术原理及相关代码的队伍，初赛成绩视为无效。    \n",
    "- 5、$\\color{blue}{9月1日关闭报名通道且最终数据结果提交截止}$。  \n",
    "- 6、$\\color{blue}{9月8日初赛排名团队公布}$。  \n",
    "- 7、初赛截止时间是9月8日18:00，同时满足以下条件的队伍进入复赛：  \n",
    "- a.在大赛官方完成注册且“昵称/姓名/邮箱/队名和队伍结构“一致。  \n",
    "- b.初赛成绩排名前5名。  \n",
    "- c.参赛队伍需要提供源代码和技术文档（技术文档应包括算法原理）。编程语言不限（java,scala, python，r均可）。  \n",
    "\n",
    "#### 复赛（2017年9月中旬）  \n",
    "- 复赛将聘请专家在以现场答辩会的形式对参赛队伍的算法原理、代码质量等进行评审，结合初赛排行榜评比出比赛前三名(排行榜的成绩占80%，专家答辩会的成绩占20%)。 \n",
    "- 获奖团队奖受邀参与高峰论坛暨颁奖仪式。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 问题描述\n",
    "叶片结冰：叶片结冰是风电领域的一个全球范围难题。$\\color{blue}{低温环境}$所导致的叶片结冰、材料及结构性能改变、载荷改变的问题等，对风机的发电性能和安全运行造成较大的威胁。随着风机的设计功率不断提升，现有风机塔筒高度也在不断增长，因此即使在北部沿海和山区地区，$\\color{blue}{冬季里大量风机都会触碰到较低的云层，在低温和潮湿环境下非常容易结冰}$。目前风机运行的实时数据主要由SCADA（注）系统进行存储，对叶片结冰故障的监测手段主要是比较风机实际功率与理论功率之间的偏差，当偏差达到一定值后会触发风机的报警和停机。然而，触发报警时往往已经发生叶片大面积结冰现象，在这样的情况下运行会增加叶片折断损坏的风险。虽然许多新型风机都设计了自动除冰系统，但是$\\color{blue}{实际应用中面临的挑战是很难对结冰的早期过程进行精确预测}$，以便能够尽早开启除冰系统。对结冰过程的预测准确度决定了除冰系统的效率、风机的效率损失和风机运行的风险。  \n",
    "\n",
    "注：SCADA是风场设备管理、监测、和控制的重要系统，通过实时收集风机运行的环境参数、工况参数、状态参数和控制参数使风场管理者能够实时了解风电装备资产的运行和健康状态。  \n",
    "\n",
    "SCADA系统每天产生大量的数据，但是$\\color{blue}{目前大部分的系统依然局限于对已发生故障的报警}$。这些故障到达报警阶段时往往已经比较严重，需要对风机进行停机和维修，造成巨大的发电损失和维护成本。通过对SCADA系统产生的大数据环境进行挖掘和建模，能够对一些严重故障进行预测和诊断，从而$\\color{blue}{使过去应激型的维护方式转变为主动预测型的维护方式}$，能够有效地改善风电设备的使用率和运维成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 评分规则\n",
    "参赛者在提交结果后，系统将根据以下规则自动对结果进行评分：\n",
    "- 故障时间区间覆盖的数据行标记为1。\n",
    "- 正常时间区间覆盖的数据行标记为0。\n",
    "- 无效数据不参与评价。\n",
    "![](doc/score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 训练与测试数据集描述\n",
    "'train'数据集中包括两个风机的数据，存在两个以风机编号命名的文件夹中。每个文件夹中的数据包括三个文件：  \n",
    "- 编号_data.csv：风机连续时间内的SCADA原始数据（详细内容如表一所示）。  \n",
    "- 编号_normalinfo.csv：风机正常状态的时间段，第一列为起始时间，第二列为结束时间。  \n",
    "- 编号_failureinfo.csv：风机结冰故障的时间段，第一列为起始时间，第二列为结束时间。  \n",
    "- 风机正常时间区间和风机结冰时间区间均不覆盖的数据视为无效数据。  \n",
    "  \n",
    "'test'测试数据集和'final'最终数据集，数据集中同样有若干个以风机编号为命名的文件夹，每个文件夹中包括一个文件：\n",
    "- 编号_data.csv：风机连续时间内的SCADA原始数据。  \n",
    "  \n",
    "需要注意的是  \n",
    "- 'train'数据集中'time'变量和一列为真实的时间戳，会存在数据不连续的情况，期间会出现停机或人为删除部分数据的情况；\n",
    "- 'test'和'final'数据集中的'time'变量为连续的数字序号，序号的排列是按照时间的先后顺序，但中间会存在由于停机等原因造成的数据不连续情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Load Data and Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Python modules:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jayhsu/work/github/JupRepo/CAICT_Competition\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%run -i 'instance/import.py'\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "def timerangeCheck(t, start, end):\n",
    "    return (t>=start) & (t<=end)\n",
    "\n",
    "def variable_summaries(var, name):  \n",
    "    with tf.name_scope('summaries_'+str(name)):  \n",
    "        mean = tf.reduce_mean(var)  \n",
    "        tf.summary.scalar('mean', mean)  \n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))  \n",
    "        tf.summary.scalar('stddev', stddev)  \n",
    "        tf.summary.scalar('max', tf.reduce_max(var))  \n",
    "        tf.summary.scalar('min', tf.reduce_min(var))  \n",
    "        tf.summary.histogram('histogram', var)  \n",
    "        \n",
    "def updateProgress(msg):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def myscore(true_y, pred_y):\n",
    "    n,p =sk.metrics.confusion_matrix(train_y, train_pred)\n",
    "    tn = n[0]\n",
    "    fp = n[1]\n",
    "    fn = p[0]\n",
    "    tp = p[1]\n",
    "    #print('tn:',tn,'fp:',fp,'fn:',fn,'fp:',fp)\n",
    "    score = 1- 0.5*(fp/(tn+fp))- 0.5*(fn/(fn+tp)) \n",
    "    #print('score',score)\n",
    "    return score, {'tn':tn,'fp':fp,'fn':fn,'tp':tp}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load input data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run -i 'load_data.py'\n",
    "train_data = pd.read_csv('data/goodformat_15.csv')\n",
    "valida_data = pd.read_csv('data/goodformat_21.csv')\n",
    "test_data = pd.read_csv('data/ice1/test/08/08_data.csv')\n",
    "column_desc=pd.read_csv('doc/columndesc.csv')\n",
    "\n",
    "#train_data = train_data.fillna(2)\n",
    "#valida_data = valida_data.fillna(2)\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "valida_data = valida_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial Exploration\n",
    "**Look at your data in as many different ways as possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据描述**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCADA系统的数据通常有上百个变量，本次竞赛的数据经过筛选保留了其中28个连续数值型变量，涵盖了\n",
    "- 风机的工况参数、\n",
    "- 环境参数和\n",
    "- 状态参数 \n",
    "等多个维度。变量的名称及说明如下表所示：     \n",
    "  \n",
    "叶片结冰比赛，此次公布的数据有两组，分别是  \n",
    "- 带有故障标签的训练数据train，和  \n",
    "- 一起公布测试数据test。  \n",
    "\n",
    "参赛团队对测试数据中的文件所代表的状态进行预测后，可以提交到竞赛网站上进行评分，每天限提交一次结果。评分系统将根据评分规则对参赛者的每一次提交反馈一个分数，但不会提供每一个文件的预测结果信息。\n",
    "在8月18日组委会将提供初赛最终数据（final），所有参赛者对这组数据仅有一次提交机会，参赛者的最终成绩将来自于对这组最终数据的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>generator_speed</th>\n",
       "      <th>power</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_direction_mean</th>\n",
       "      <th>yaw_position</th>\n",
       "      <th>yaw_speed</th>\n",
       "      <th>pitch1_angle</th>\n",
       "      <th>pitch2_angle</th>\n",
       "      <th>pitch3_angle</th>\n",
       "      <th>pitch1_speed</th>\n",
       "      <th>pitch2_speed</th>\n",
       "      <th>pitch3_speed</th>\n",
       "      <th>pitch1_moto_tmp</th>\n",
       "      <th>pitch2_moto_tmp</th>\n",
       "      <th>pitch3_moto_tmp</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>environment_tmp</th>\n",
       "      <th>int_tmp</th>\n",
       "      <th>pitch1_ng5_tmp</th>\n",
       "      <th>pitch2_ng5_tmp</th>\n",
       "      <th>pitch3_ng5_tmp</th>\n",
       "      <th>pitch1_ng5_DC</th>\n",
       "      <th>pitch2_ng5_DC</th>\n",
       "      <th>pitch3_ng5_DC</th>\n",
       "      <th>group</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/11/1 20:20</td>\n",
       "      <td>1.859993</td>\n",
       "      <td>1.223595</td>\n",
       "      <td>2.515790</td>\n",
       "      <td>-2.072739</td>\n",
       "      <td>-2.073627</td>\n",
       "      <td>-0.655343</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.506667</td>\n",
       "      <td>0.551111</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.590</td>\n",
       "      <td>-1.023986</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.403919</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1</td>\n",
       "      <td>1446380416</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/11/1 20:20</td>\n",
       "      <td>1.911625</td>\n",
       "      <td>1.293394</td>\n",
       "      <td>2.313551</td>\n",
       "      <td>-2.010591</td>\n",
       "      <td>-1.615140</td>\n",
       "      <td>-0.655343</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.225767</td>\n",
       "      <td>-1.209522</td>\n",
       "      <td>-0.421277</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.88</td>\n",
       "      <td>-2.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1446380423</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/11/1 20:20</td>\n",
       "      <td>1.635027</td>\n",
       "      <td>1.280099</td>\n",
       "      <td>2.507799</td>\n",
       "      <td>-2.053750</td>\n",
       "      <td>-0.282742</td>\n",
       "      <td>-0.649566</td>\n",
       "      <td>0.170338</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-1.84</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.061109</td>\n",
       "      <td>-0.421277</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.123077</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1</td>\n",
       "      <td>1446380430</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              time  wind_speed  generator_speed     power  wind_direction  \\\n",
       "0  2015/11/1 20:20    1.859993         1.223595  2.515790       -2.072739   \n",
       "1  2015/11/1 20:20    1.911625         1.293394  2.313551       -2.010591   \n",
       "2  2015/11/1 20:20    1.635027         1.280099  2.507799       -2.053750   \n",
       "\n",
       "   wind_direction_mean  yaw_position  yaw_speed  pitch1_angle  pitch2_angle  \\\n",
       "0            -2.073627     -0.655343   0.030804      0.555556      0.506667   \n",
       "1            -1.615140     -0.655343   0.030804      0.195556      0.133333   \n",
       "2            -0.282742     -0.649566   0.170338      0.964444      0.951111   \n",
       "\n",
       "   pitch3_angle  pitch1_speed  pitch2_speed  pitch3_speed  pitch1_moto_tmp  \\\n",
       "0      0.551111         -1.68         -1.72         -1.68            0.759   \n",
       "1      0.191111          0.00          0.00          0.00            0.769   \n",
       "2      0.960000         -1.88         -1.84         -1.64            0.769   \n",
       "\n",
       "   pitch2_moto_tmp  pitch3_moto_tmp     acc_x     acc_y  environment_tmp  \\\n",
       "0            0.600            0.590 -1.023986  0.061109        -0.403919   \n",
       "1            0.609            0.600  1.225767 -1.209522        -0.421277   \n",
       "2            0.609            0.609  0.100890  0.061109        -0.421277   \n",
       "\n",
       "    int_tmp  pitch1_ng5_tmp  pitch2_ng5_tmp  pitch3_ng5_tmp  pitch1_ng5_DC  \\\n",
       "0  0.014918        1.307692        1.123077        0.783077           1.36   \n",
       "1 -0.002291        1.307692        1.123077        0.783077           0.44   \n",
       "2 -0.002291        1.307692        1.123077        0.783077           1.76   \n",
       "\n",
       "   pitch2_ng5_DC  pitch3_ng5_DC  group   timestamp  label  \n",
       "0           0.00           1.56      1  1446380416    0.0  \n",
       "1           2.88          -2.60      1  1446380423    0.0  \n",
       "2           0.60           2.56      1  1446380430    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training Data (風機編號15) 風機狀態正常(0)/異常(1)比例 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    350255\n",
       "1.0     23892\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_=train_data['label'].value_counts()\n",
    "_\n",
    "train_abnormal = train_data[train_data['label']==1]\n",
    "train_normal = train_data[train_data['label']==0]\n",
    "normal_col = \"green\"\n",
    "abnormal_col = \"red\"\n",
    "\n",
    "msg = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(train_normal), j=len(train_normal)/len(train_data),k=len(train_abnormal),m=len(train_abnormal)/len(train_data), n=len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Validation Data (風機編號21) 風機狀態正常(0)/異常(1)比例 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    168930\n",
       "1.0     10638\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常:168930 (0.94 percent), 結冰:10638 (0.06 percent), Total:179568\n"
     ]
    }
   ],
   "source": [
    "_=valida_data['label'].value_counts()\n",
    "_\n",
    "valida_abnormal = valida_data[valida_data['label']==1]\n",
    "valida_normal = valida_data[valida_data['label']==0]\n",
    "\n",
    "msg = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(valida_normal), j=len(valida_normal)/len(valida_data),k=len(valida_abnormal),m=len(valida_abnormal)/len(valida_data), n=len(valida_data))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 觀察：\n",
    "- Data Imbalance!!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 根據training process需求, 將train_data拆分為train_X, train_Y, 並轉換為numpy.ndarray的格式 **  \n",
    "** 因為目標是做風機normal/abnormal的分類預測, 所以把label轉換為one-hot representation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = train_data.drop(['label','time','group','timestamp'], axis=1)\n",
    "train_X = tmp.values\n",
    "train_y = train_data['label'].values\n",
    "train_y = train_y.astype(int)\n",
    "\n",
    "tmp = valida_data.drop(['label','time','group','timestamp'], axis=1)\n",
    "validation_X= tmp.values\n",
    "validation_y = valida_data['label'].values.astype(int)\n",
    "\n",
    "test_timeidx = test_data['time']\n",
    "tmp = test_data.drop(['time','group'], axis=1)\n",
    "test_X = tmp.values\n",
    "\n",
    "\n",
    "#smote = SMOTE()\n",
    "#train_X, train_y = smote.fit_sample(train_X,train_y)\n",
    "\n",
    "train_Y = np.eye(3)[train_y]\n",
    "validation_Y = np.eye(3)[validation_y]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01 # learning rate\n",
    "n_inputs = 26 # 每一行的维度\n",
    "n_classes = 3  # RNN最后的输出類別個数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tensorflow 參數定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X =tf.placeholder(tf.float32, [None, n_inputs], name=\"X\")\n",
    "Y_ =tf.placeholder(tf.float32, [None, n_classes], name=\"Y_\")\n",
    "\n",
    "W = {\n",
    "    'out_h0': tf.Variable(tf.random_normal([n_inputs,n_classes], stddev=0.01)),\n",
    "    'wd_h2_1': tf.Variable(tf.random_normal([n_inputs,20], stddev=0.01)),\n",
    "    'wd_h2_2': tf.Variable(tf.random_normal([20,15], stddev=0.01)),\n",
    "    'wd_h2_3': tf.Variable(tf.random_normal([15,10], stddev=0.01)),\n",
    "    'wd_h2_4': tf.Variable(tf.random_normal([10,5], stddev=0.01)),\n",
    "    'out_h2': tf.Variable(tf.random_normal([5, n_classes]), name=\"out\")\n",
    "}\n",
    "#variable_summaries(W['wd1'],'wd1')\n",
    "#variable_summaries(W['wd2'],'wd2')\n",
    "#variable_summaries(W['out'],'out')\n",
    "\n",
    "B = {\n",
    "    'out_h0': tf.Variable(tf.random_normal([n_classes]), name=\"out\"),\n",
    "    'bd_h2_1': tf.Variable(tf.random_normal([20])),\n",
    "    'bd_h2_2': tf.Variable(tf.random_normal([15])),\n",
    "    'bd_h2_3': tf.Variable(tf.random_normal([10])),\n",
    "    'bd_h2_4': tf.Variable(tf.random_normal([5])),\n",
    "    'out_h2': tf.Variable(tf.random_normal([n_classes]), name=\"out\"),\n",
    "}\n",
    "\n",
    "#variable_summaries(B['bd1'],'bd1')\n",
    "#variable_summaries(B['bd2'],'bd2')\n",
    "#variable_summaries(B['out'],'out')\n",
    "\n",
    "#tfdot() 不看!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 網路結構 Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Hidden_Layer1\"):\n",
    "    H1 = tf.matmul(X, W['wd_h2_1']) + B['bd_h2_1'] \n",
    "    H1 = tf.nn.relu(H1)\n",
    "\n",
    "with tf.name_scope(\"Hidden_Layer2\"):\n",
    "    H2 = tf.matmul(H1, W['wd_h2_2']) + B['bd_h2_2'] # (-1, 28) matmul (28, hidden_units) => (-1, hidden_units)\n",
    "    H2 = tf.nn.relu(H2)\n",
    "    \n",
    "with tf.name_scope(\"Hidden_Layer3\"):\n",
    "    H3 = tf.matmul(H2, W['wd_h2_3']) + B['bd_h2_3'] \n",
    "    H3 = tf.nn.relu(H3)\n",
    "\n",
    "with tf.name_scope(\"Hidden_Layer4\"):\n",
    "    H4 = tf.matmul(H3, W['wd_h2_4']) + B['bd_h2_4'] # (-1, 28) matmul (28, hidden_units) => (-1, hidden_units)\n",
    "    H5 = tf.nn.relu(H4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with tf.name_scope('outlayer_H0'):\n",
    "#    _pred = tf.matmul(X, W['out_h0']) + B['out_h0']\n",
    "#    pred = tf.nn.softmax(_pred, name=\"pred\")\n",
    "\n",
    "with tf.name_scope('outlayer_out'):\n",
    "    _pred = tf.matmul(H5, W['out_h2']) + B['out_h2']\n",
    "    pred = tf.nn.softmax(_pred, name=\"pred\")\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 誤差函數 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tmp = train_data['label'].value_counts()\n",
    "#w1= tmp[2]/len(train_data)\n",
    "#w2=(1-tmp[2]/len(train_data))/2\n",
    "#class_weight = tf.constant([[w1, w2,w2]])\n",
    "#weighted_logits = tf.multiply(_pred, class_weight) # shape [batch_size, 2]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_pred, labels=Y_))\n",
    "#loss = tf.losses.sparse_softmax_cross_entropy(labels=Y_, logits=_pred, weights=1.0)\n",
    "\n",
    "#weight_per_label = tf.transpose( tf.matmul(Y_, tf.transpose(class_weight)))\n",
    "#xent = tf.multiply(weight_per_label, tf.nn.softmax_cross_entropy_with_logits(logits=_pred, labels=Y_))\n",
    "#loss = tf.reduce_mean(xent)\n",
    "\n",
    "# this is the weight for each datapoint, depending on its label\n",
    "#xent = tf.mul(weight_per_label, tf.nn.softmax_cross_entropy_with_logits(_pred, Y_, name=\"xent_raw\")  \n",
    "#loss = tf.reduce_mean(xent)\n",
    "\n",
    "              \n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimizer (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 開始訓練  Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "#with tf.name_scope('performance'):  \n",
    "#    loss_scar = tf.summary.scalar('loss', loss)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "#    acc_scar = tf.summary.scalar('accuracy', accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "#merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先創造一個session, 然後記得要init variable\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "#if not os.path.exists('tb-log_1'):\n",
    "#    os.mkdir('tb-log_1')\n",
    "#summary_writer_train = tf.summary.FileWriter('tb-log1/train',graph=tf.get_default_graph())\n",
    "#summary_writer_validation = tf.summary.FileWriter('tb-log1/validation',graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, batch:374, train loss:0.301 acc:0.930, test loss:0.272 acc:0.941\n",
      "epoch:1, batch:374, train loss:0.230 acc:0.945, test loss:0.242 acc:0.941\n",
      "epoch:2, batch:374, train loss:0.258 acc:0.932, test loss:0.234 acc:0.941\n",
      "epoch:3, batch:374, train loss:0.276 acc:0.924, test loss:0.231 acc:0.941\n",
      "epoch:4, batch:374, train loss:0.225 acc:0.942, test loss:0.229 acc:0.941\n",
      "epoch:5, batch:374, train loss:0.235 acc:0.938, test loss:0.228 acc:0.941\n",
      "epoch:6, batch:374, train loss:0.235 acc:0.938, test loss:0.227 acc:0.941\n",
      "epoch:7, batch:374, train loss:0.199 acc:0.951, test loss:0.227 acc:0.941\n",
      "epoch:8, batch:374, train loss:0.234 acc:0.938, test loss:0.226 acc:0.941\n",
      "epoch:9, batch:374, train loss:0.252 acc:0.931, test loss:0.226 acc:0.941\n",
      "epoch:10, batch:374, train loss:0.260 acc:0.928, test loss:0.226 acc:0.941\n",
      "epoch:11, batch:374, train loss:0.236 acc:0.937, test loss:0.226 acc:0.941\n",
      "epoch:12, batch:374, train loss:0.252 acc:0.931, test loss:0.226 acc:0.941\n",
      "epoch:13, batch:374, train loss:0.238 acc:0.936, test loss:0.225 acc:0.941\n",
      "epoch:14, batch:374, train loss:0.222 acc:0.942, test loss:0.225 acc:0.941\n",
      "epoch:15, batch:374, train loss:0.262 acc:0.927, test loss:0.225 acc:0.941\n",
      "epoch:16, batch:374, train loss:0.233 acc:0.938, test loss:0.225 acc:0.941\n",
      "epoch:17, batch:374, train loss:0.241 acc:0.935, test loss:0.225 acc:0.941\n",
      "epoch:18, batch:374, train loss:0.214 acc:0.945, test loss:0.225 acc:0.941\n",
      "epoch:19, batch:374, train loss:0.222 acc:0.942, test loss:0.225 acc:0.941\n",
      "epoch:20, batch:374, train loss:0.270 acc:0.924, test loss:0.225 acc:0.941\n"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "epoch = 20\n",
    "batch_size = 1000\n",
    "total_batch= len(train_X) / batch_size\n",
    "for ep in range(epoch+1):\n",
    "    for i in range(int(total_batch)+1):\n",
    "        \n",
    "        rnd_idx = np.random.choice(train_X.shape[0], batch_size, replace=False)\n",
    "        batch_x = train_X[rnd_idx]\n",
    "        batch_y = train_Y[rnd_idx]\n",
    "        _, acc_v1, loss_v1= sess.run([optimizer, accuracy,loss], feed_dict={X: batch_x, Y_:batch_y})\n",
    "        #summary_writer_train.add_summary(summary, ep * total_batch + i)\n",
    "        #if i%100 ==0:\n",
    "    acc_v2, loss_v2= sess.run([accuracy,loss], feed_dict={X: validation_X , Y_: validation_Y})\n",
    "    #summary_writer_validation.add_summary(summary, ep * total_batch + i)\n",
    "    updateProgress('epoch:{x0}, batch:{x4}, train loss:{x1:.3f} acc:{x5:.3f}, test loss:{x3:.3f} acc:{x2:.3f}'.format(x0=ep,x2=round(acc_v2,3),x3=round(loss_v2,3),x4=i,x1=round(loss_v1,3), x5=round(acc_v1,3)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 預測與準確率評估  Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.936143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    350255\n",
       "1     23892\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    374147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.0\n",
      "Recall 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayhsu/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# predict all X and get the accuracy\n",
    "_ = accuracy.eval({X: train_X , Y_: train_Y})\n",
    "print('Accuracy:',_)\n",
    "pred_ = pd.DataFrame(pred.eval({X: train_X}))\n",
    "train_pred = pred_.apply(np.argmax,axis=1)\n",
    "pd.Series(train_y).value_counts()\n",
    "train_pred.value_counts()\n",
    "print(\"Precision\", precision_score(train_y, train_pred))\n",
    "print(\"Recall\", sk.metrics.recall_score(train_y, train_pred))\n",
    "\n",
    "s,_=myscore(train_y, train_pred)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict  Validataion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.940758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    179568\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.0\n",
      "Recall 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayhsu/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# predict all X and get the accuracy\n",
    "_ = accuracy.eval({X: validation_X , Y_: validation_Y})\n",
    "print('Accuracy:',_)\n",
    "pred_ = pd.DataFrame(pred.eval({X: validation_X}))\n",
    "validation_pred = pred_.apply(np.argmax,axis=1)\n",
    "validation_pred.value_counts()\n",
    "print(\"Precision\", precision_score(validation_y, validation_pred))\n",
    "print(\"Recall\", sk.metrics.recall_score(validation_y, validation_pred))\n",
    "\n",
    "s,_=myscore(train_y, train_pred)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_ = pd.DataFrame(pred.eval({X: test_X}))\n",
    "test_pred = pred_.apply(np.argmax,axis=1)\n",
    "test_pred.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 整理預測結果, 正確上傳格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime=0\n",
    "endTIme=0\n",
    "search_start=True\n",
    "search_end=False\n",
    "abnormal_list=[]\n",
    "for i,v in enumerate(test_pred_out):\n",
    "    if (v==1) & (search_start):\n",
    "        startTime = test_timeidx[i]\n",
    "        search_end=True\n",
    "        search_start=False\n",
    "    if (v==0) & (search_end):\n",
    "        endTIme=test_timeidx[i]\n",
    "        search_start=True\n",
    "        search_end=False\n",
    "        abnormal_list.append((startTime,test_timeidx[i-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_ans=False\n",
    "if(output_ans):\n",
    "    with open('test1_08_results.csv','w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        #csv_out.writerow(['startTime','endTime'])\n",
    "        for row in abnormal_list:\n",
    "            csv_out.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Issue\n",
    "1. Data Imbalance\n",
    " - training batch sample 一半normal一半abnormal的data\n",
    " - 用兩台風機訓練兩個model, 取故障預測的聯集 \n",
    "\n",
    "2. 用Precision & Recall評估好壞, 算出Score\n",
    "\n",
    "\n",
    "other:\n",
    "- using weighted examples. Just amplify the per-instance loss by a larger weight when seeing positive examples. If you use online gradient descent, it can be as simple as using a larger learning rate when seeing positive examples.\n",
    "- A similar and slightly better approach (only if you use stochastic gradient descent) is randomly picking an example in each iteration, where the positive examples have higher probability of being picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "提交结果历史记录  \n",
    "参赛队伍:　556 / 参赛人数:　858  \n",
    " \n",
    "|竞赛阶段\t |上传者\t |分数\t |提交日期\t |排名\t |下载|\n",
    "| ---------| -------- | ------ | ------ | ------ | ------ |\n",
    "|初赛test1阶段|\t孔祥千\t|55.28684214|\t2017/7/12|\t50\t|下载|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 7/13\n",
    "1. cross correlation\n",
    "2. auto correlation\n",
    "3. common filter\n",
    "4. de-train "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

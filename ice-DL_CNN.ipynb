{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAICT 風機結冰預測 - DL_CNN_TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017/07/24  \n",
    "http://www.industrial-bigdata.com/competition/competitionAction!showDetail.action?competition.competitionId=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Python modules:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import lzma\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import shutil\n",
    "import csv\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** utils **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):  \n",
    "    with tf.name_scope('summaries_'+str(name)):  \n",
    "        mean = tf.reduce_mean(var)  \n",
    "        tf.summary.scalar('mean', mean)  \n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))  \n",
    "        tf.summary.scalar('stddev', stddev)  \n",
    "        tf.summary.scalar('max', tf.reduce_max(var))  \n",
    "        tf.summary.scalar('min', tf.reduce_min(var))  \n",
    "        tf.summary.histogram('histogram', var)  \n",
    "        \n",
    "def updateProgress(msg):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_selection(data, isTest=False):\n",
    "    if isTest:\n",
    "        fixed_variable = ['time','group']\n",
    "    else:\n",
    "        fixed_variable = ['label','event','time','group','timestamp']\n",
    "    #selected_variable = ['pitch1_angle','pitch2_angle','pitch3_angle','pitch1_speed','pitch3_speed','pitch2_speed']\n",
    "    selected_variable = ['pitch1_angle','pitch2_angle','pitch3_angle','pitch1_speed','pitch3_speed','pitch2_speed','power','yaw_position','environment_tmp']\n",
    "    variable = selected_variable + fixed_variable \n",
    "    data = data[variable]\n",
    "    return data\n",
    "\n",
    "def load_resample_data(id, remove_unknown=True, split_case=20, split_validation=True, policy=1, varsel=False, label=None):\n",
    "    filename = 'data/resample_data_p{POLICY}_{ID}.csv'.format(ID=id,POLICY=policy)\n",
    "    data_resample = pd.read_csv(filename)\n",
    "    if varsel:\n",
    "        data_resample=variable_selection(data_resample)\n",
    "    \n",
    "    if remove_unknown:\n",
    "        data_resample = data_resample[data_resample['label']!=2]\n",
    "    \n",
    "    if label is not None:\n",
    "        data_resample = data_resample[data_resample['label']==label]\n",
    "    \n",
    "    data_resample_train = data_resample[data_resample['event']<split_case]\n",
    "    data_resample_valid = data_resample[data_resample['event']>=split_case]\n",
    "\n",
    "    train_aX = data_resample.drop(['label','time','group','timestamp','event'], axis=1)\n",
    "    train_ay = data_resample['label']\n",
    "    train_aY = np.eye(2)[train_ay.values.astype(int)]\n",
    "    \n",
    "    train_X = data_resample_train.drop(['label','time','group','timestamp','event'], axis=1)\n",
    "    train_y = data_resample_train['label']\n",
    "    valid_X = data_resample_valid.drop(['label','time','group','timestamp','event'], axis=1)\n",
    "    valid_y = data_resample_valid['label']\n",
    "    train_Y = np.eye(2)[train_y.values.astype(int)]\n",
    "    valid_Y = np.eye(2)[valid_y.values.astype(int)]\n",
    "\n",
    "    train_abnormal = data_resample_train[data_resample_train['label']==1]\n",
    "    train_normal = data_resample_train[data_resample_train['label']==0]\n",
    "    train_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(train_normal), j=len(train_normal)/len(data_resample_train),k=len(train_abnormal),m=len(train_abnormal)/len(data_resample_train), n=len(data_resample_train))\n",
    "    \n",
    "    valid_abnormal = data_resample_valid[data_resample_valid['label']==1]\n",
    "    valid_normal = data_resample_valid[data_resample_valid['label']==0]\n",
    "    valid_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(valid_normal), j=len(valid_normal)/len(data_resample_valid),k=len(valid_abnormal),m=len(valid_abnormal)/len(data_resample_valid), n=len(data_resample_valid))\n",
    "    if split_validation:\n",
    "        return data_resample, train_X, train_Y, train_y, valid_X, valid_Y, valid_y, train_desc, valid_desc\n",
    "    else:\n",
    "        return data_resample, train_aX, train_aY, train_ay\n",
    "\n",
    "def load_test1_data(id,remove_unknown=True, dropna=True, varsel=False):\n",
    "    filename = 'data/goodformat_{ID}.csv'.format(ID=id)\n",
    "    data_raw = pd.read_csv(filename)\n",
    "    if varsel:\n",
    "        data_raw=variable_selection(data_raw)\n",
    "        \n",
    "    if dropna:\n",
    "        data_raw = data_raw.dropna()\n",
    "    else: \n",
    "        data_raw = data_raw.fillna(0)\n",
    "        \n",
    "    data_X = data_raw.drop(['label','time','group','timestamp'], axis=1).values\n",
    "    data_y = data_raw['label'].values.astype(int)\n",
    "    data_Y = np.eye(2)[data_y]\n",
    "    \n",
    "    data_abnormal = data_raw[data_raw['label']==1]\n",
    "    data_normal = data_raw[data_raw['label']==0]\n",
    "    data_desc = '正常:{i} ({j:.2f} percent), 結冰:{k:} ({m:.2f} percent), Total:{n}'.format(i=len(data_normal), j=len(data_normal)/len(data_raw),k=len(data_abnormal),m=len(data_abnormal)/len(data_raw), n=len(data_raw))\n",
    "    \n",
    "    return data_raw, data_X, data_Y, data_y, data_desc\n",
    "\n",
    "def load_test2_data(varsel=False):\n",
    "    test_data = pd.read_csv('data/ice1/test/08/08_data.csv')\n",
    "    if varsel:\n",
    "        test_data=variable_selection(test_data,isTest=True)\n",
    "    test_timeidx = test_data['time']\n",
    "    tmp = test_data.drop(['time','group'], axis=1)\n",
    "    test_X = tmp\n",
    "    return test_data, test_X, test_timeidx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myscore(true_y, pred_y):\n",
    "    n,p =sk.metrics.confusion_matrix(true_y, pred_y)\n",
    "    tn = n[0]\n",
    "    fp = n[1]\n",
    "    fn = p[0]\n",
    "    tp = p[1]\n",
    "    score = 1- 0.5*(fp/(tn+fp))- 0.5*(fn/(fn+tp)) \n",
    "    prec= precision_score(true_y, pred_y)\n",
    "    recall = recall_score(true_y, pred_y)\n",
    "    accuracy = accuracy_score(true_y,pred_y)\n",
    "    confusions = (\n",
    "    ('tn', tn),\n",
    "    ('fn', fn),\n",
    "    ('tp', tp),\n",
    "    ('fp', fp)\n",
    "    )\n",
    "    msg = 'Score:{sco} \\n\\taccuracy:{acc}, \\n\\trecall:{rcl}, \\n\\tprecision:{psc}\\n\\t{cfmt}'.format(sco=score,acc=accuracy,rcl=recall,psc=prec,cfmt=confusions)\n",
    "    return score, accuracy, prec, recall, OrderedDict(confusions),msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load input data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: data is in DataFrame format \n",
    "train_raw, train_X, train_Y, train_y, valid_X, valid_Y, valid_y, train_desc, valid_desc = load_resample_data(id=15,policy=1,varsel=False)\n",
    "valid_raw, valid_21_X, valid_21_Y, valid_21_y= load_resample_data(id=21,policy=0,split_validation=False,split_case=8,varsel=False)\n",
    "test_raw, test_X,test_timeidx = load_test2_data(varsel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15-train: 正常:35339 (0.61 percent), 結冰:22179 (0.39 percent), Total:57518\n",
      "15-valid: 正常:7680 (0.64 percent), 結冰:4315 (0.36 percent), Total:11995\n"
     ]
    }
   ],
   "source": [
    "print('15-train:', train_desc)\n",
    "print('15-valid:',valid_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 為了CNN需求, 更改 input X format **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_dataset(data, n_steps):\n",
    "    dataset_new = []\n",
    "    for idx, _ in enumerate(data):\n",
    "        start_idx = idx-n_steps\n",
    "        end_idx = idx+1\n",
    "        if start_idx<0:\n",
    "            continue\n",
    "        n=np.concatenate(data[start_idx:end_idx], axis=0)\n",
    "        dataset_new.append(n)\n",
    "    dataset_new = np.asarray(dataset_new)\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_steps_=n_steps-1\n",
    "train_X_new = cnn_dataset(train_X.values,n_steps_)\n",
    "valid_X_new = cnn_dataset(valid_X.values,n_steps_)\n",
    "valid_21_X_new = cnn_dataset(valid_21_X.values,n_steps_)\n",
    "test_X = cnn_dataset(test_X.values,n_steps_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202309, 520)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_new=train_Y[n_steps_:]\n",
    "train_y_new=train_y[n_steps_:]\n",
    "valid_Y_new=valid_Y[n_steps_:]\n",
    "valid_y_new=valid_y[n_steps_:]\n",
    "valid_21_Y_new=valid_21_Y[n_steps_:]\n",
    "valid_21_y_new=valid_21_y[n_steps_:]\n",
    "test_timeidx_new=test_timeidx[n_steps_:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202309,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_timeidx_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Parameters **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X =tf.placeholder(tf.float32, [None, n_steps, n_inputs], name=\"X\")\n",
    "Y_ =tf.placeholder(tf.float32, [None, n_classes], name=\"Y_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** CNN Network Structure **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    \n",
    "    \n",
    "    W_e_1 = weight_variable([784, 500], \"w_e_1\")\n",
    "    b_e_1 = bias_variable([500], \"b_e_1\")\n",
    "    h_e_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, W_e_1), b_e_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tensorflow 參數定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_inputs = len(train_X[1]) # 每一行的维度\n",
    "n_hidden_unins = 13 # 中間RNN的hidden units\n",
    "n_classes = len(train_Y[1])  # RNN最后的输出類別個数\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X =tf.placeholder(tf.float32, [None, n_steps, n_inputs], name=\"X\")\n",
    "Y_ =tf.placeholder(tf.float32, [None, n_classes], name=\"Y_\")\n",
    "\n",
    "W = {\n",
    "    'in': tf.Variable(tf.random_uniform([n_inputs, n_hidden_unins], -1.0, 1.0), name=\"In_W\"), # 输入层到中间层权重矩阵\n",
    "    'out': tf.Variable(tf.random_uniform([n_hidden_unins, n_classes], -1.0, 1.0), name=\"Out_W\"), # 中间层到输出层的权重\n",
    "}\n",
    "\n",
    "variable_summaries(W['in'],'In_W')\n",
    "variable_summaries(W['out'],'Out_W')\n",
    "B = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_unins]), name=\"In_Bias\"), # 偏置\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes]), name=\"Out_Bias\"),\n",
    "}\n",
    "\n",
    "variable_summaries(B['in'],'In_Bias')\n",
    "variable_summaries(B['out'],'OUT_Bias')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 網路結構 Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden_layer for input\n",
    "# X : (128, 28, 28)\n",
    "with tf.name_scope(\"inLayer\"):\n",
    "    X_ = tf.reshape(X, [-1, n_inputs])  # [batch_size*28, 28]\n",
    "    X_in = tf.matmul(X_, W['in']) + B['in'] # (-1, 28) matmul (28, hidden_units) => (-1, hidden_units)\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_unins]) # (batch_size, steps, input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tf.contrib.rnn.GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"RNN_CELL\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_unins) \n",
    "    #rnn_outputs, rnn_states = tf.nn.dynamic_rnn(lstm_cell, X_in, dtype=tf.float32) \n",
    "    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(lstm_cell, X_in, dtype=tf.float32) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('outlayer'):\n",
    "    _pred = tf.matmul(rnn_states[0], W['out']) + B['out']\n",
    "    pred = tf.nn.softmax(_pred, name=\"pred\")\n",
    "    #rnn_outputs = tf.unstack(tf.transpose(rnn_outputs, [1,0,2]))  \n",
    "    #pred = tf.matmul(rnn_outputs[-1], weights['out']) + biases['out'] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 誤差函數 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_pred, labels=Y_))\n",
    "    loss_scar = tf.summary.scalar('loss', loss)\n",
    "    acc_scar = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimizer (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001 # learning rate\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 開始訓練  Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先創造一個session, 然後記得要init variable\n",
    "init = tf.global_variables_initializer()\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "#if not os.path.exists('tb-log_1'):\n",
    "#    os.mkdir('tb-log_1')\n",
    "#summary_writer_train = tf.summary.FileWriter('tb-log1/train',graph=tf.get_default_graph())\n",
    "#summary_writer_validation = tf.summary.FileWriter('tb-log1/validation',graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-a87c7b351485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_Y_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrnd_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_v1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_v1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[1;31m#summary_writer_train.add_summary(summary, ep * total_batch + i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\foxconn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\foxconn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\foxconn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\foxconn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\foxconn\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "total_batch= len(train_X) / batch_size\n",
    "for ep in range(epoch+1):\n",
    "    for i in range(int(total_batch)+1):\n",
    "        rnd_idx = np.random.choice(train_X_new.shape[0], batch_size, replace=False)\n",
    "        batch_x = train_X_new[rnd_idx]\n",
    "        batch_y = train_Y_new[rnd_idx]\n",
    "        batch_x = batch_x.reshape([batch_size, n_steps, n_inputs])\n",
    "        _, loss_v1, acc_v1= sess.run([optimizer, loss, accuracy], feed_dict={X: batch_x, Y_:batch_y})\n",
    "        #summary_writer_train.add_summary(summary, ep * total_batch + i)\n",
    "\n",
    "    batch_vx = valid_X_new.reshape([-1, n_steps, n_inputs])\n",
    "    loss_v2, acc_v2= sess.run([loss, accuracy], feed_dict={X: batch_vx , Y_: valid_Y_new})\n",
    "    #summary_writer_validation.add_summary(summary, ep * total_batch + i)\n",
    "    updateProgress('epoch:{x0}, batch:{x4}, train loss:{x1:.3f} acc:{x5:.3f}, test loss:{x3:.3f} acc:{x2:.3f}'.format(x0=ep,x2=round(acc_v2,3),x3=round(loss_v2,3),x4=i,x1=round(loss_v1,3), x5=round(acc_v1,3)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 預測與準確率評估  Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    35339\n",
       "1    22179\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    35226\n",
       "1    22093\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.989363146698\n",
      "Recall 0.985526849723\n",
      "score 0.989419657075 {'tn': 34905, 'fn': 321, 'tp': 21858, 'fp': 235}\n"
     ]
    }
   ],
   "source": [
    "# predict all X and get the accuracy\n",
    "train_X_rs = train_X_new.reshape([-1, n_steps, n_inputs])\n",
    "acc_t = accuracy.eval({X: train_X_rs , Y_: train_Y_new})\n",
    "print('Accuracy:',acc_t)\n",
    "\n",
    "pred_t = pd.DataFrame(pred.eval({X: train_X_rs}))\n",
    "pred_t = pred_t.apply(np.argmax,axis=1)\n",
    "pd.Series(train_y).value_counts()\n",
    "pred_t.value_counts()\n",
    "\n",
    "print(\"Precision\", precision_score(train_y_new, pred_t))\n",
    "print(\"Recall\", sk.metrics.recall_score(train_y_new, pred_t))\n",
    "\n",
    "s,_=myscore(train_y_new, pred_t)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict  Validataion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.673279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    7363\n",
       "1    4433\n",
       "dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.551996390706\n",
      "Recall 0.567091541136\n",
      "score 0.650809505363 {'tn': 5495, 'fn': 1868, 'tp': 2447, 'fp': 1986}\n"
     ]
    }
   ],
   "source": [
    "# predict all X and get the accuracy\n",
    "valid_X_rs = valid_X_new.reshape([-1, n_steps, n_inputs])\n",
    "acc_v = accuracy.eval({X: valid_X_rs , Y_: valid_Y_new})\n",
    "print('Accuracy:',acc_v)\n",
    "\n",
    "pred_v = pd.DataFrame(pred.eval({X: valid_X_rs}))\n",
    "pred_v = pred_v.apply(np.argmax,axis=1)\n",
    "pred_v.value_counts()\n",
    "\n",
    "print(\"Precision\", precision_score(valid_y_new, pred_v))\n",
    "print(\"Recall\", sk.metrics.recall_score(valid_y_new, pred_v))\n",
    "\n",
    "s,_=myscore(valid_y_new, pred_v)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predict another machine **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    85271\n",
       "1    14729\n",
       "dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.901398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    71886\n",
       "1     7483\n",
       "dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(179369,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    157157\n",
       "1     22212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.188096524401\n",
      "Recall 0.392742996804\n",
      "score 0.642931407369 {'tn': 150697, 'fn': 6460, 'tp': 4178, 'fp': 18034}\n"
     ]
    }
   ],
   "source": [
    "# predict all X and get the accuracy\n",
    "valid_21_X_rs = valid_21_X_new.reshape([-1, n_steps, n_inputs])\n",
    "\n",
    "valid_21_X_rs__1=valid_21_X_rs[:100000] \n",
    "valid_21_Y_new__1=valid_21_Y_new[:100000] \n",
    "valid_21_y_new__1 = valid_21_y_new[:100000] \n",
    "acc_t1 = accuracy.eval({X: valid_21_X_rs__1 , Y_: valid_21_Y_new__1})\n",
    "print('Accuracy:',acc_t1)\n",
    "pred_t1 = pd.DataFrame(pred.eval({X: valid_21_X_rs__1}))\n",
    "pred_t1 = pred_t1.apply(np.argmax,axis=1)\n",
    "pred_t1.value_counts()\n",
    "\n",
    "\n",
    "valid_21_X_rs__2=valid_21_X_rs[100000:] \n",
    "valid_21_Y_new__2=valid_21_Y_new[100000:] \n",
    "valid_21_y_new__2 = valid_21_y_new[100000:] \n",
    "\n",
    "acc_t2 = accuracy.eval({X: valid_21_X_rs__2 , Y_: valid_21_Y_new__2})\n",
    "print('Accuracy:',acc_t2)\n",
    "pred_t2 = pd.DataFrame(pred.eval({X: valid_21_X_rs__2}))\n",
    "pred_t2 = pred_t2.apply(np.argmax,axis=1)\n",
    "pred_t2.value_counts()\n",
    "\n",
    "pred_t = pred_t1.append(pred_t2)\n",
    "pred_t.shape\n",
    "pred_t.value_counts()\n",
    "\n",
    "print(\"Precision\", precision_score(valid_21_y_new, pred_t))\n",
    "print(\"Recall\", sk.metrics.recall_score(valid_21_y_new, pred_t))\n",
    "\n",
    "s,_=myscore(valid_21_y_new, pred_t)\n",
    "print('score',s,_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    52413\n",
       "1    47587\n",
       "dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    85767\n",
       "1    16362\n",
       "dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_rs = test_X.reshape([-1, n_steps, n_inputs])\n",
    "test_X_rs__1 = test_X_rs[:100000]\n",
    "pred_T1 = pd.DataFrame(pred.eval({X: test_X_rs__1}))\n",
    "pred_T1 = pred_T1.apply(np.argmax,axis=1)\n",
    "pred_T1.value_counts()\n",
    "\n",
    "test_X_rs__2 = test_X_rs[100000:]\n",
    "pred_T2 = pd.DataFrame(pred.eval({X: test_X_rs__2}))\n",
    "pred_T2 = pred_T2.apply(np.argmax,axis=1)\n",
    "pred_T2.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202129,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    138180\n",
       "1     63949\n",
       "dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_T = pred_T1.append(pred_T2)\n",
    "pred_T.shape\n",
    "pred_T.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 整理預測結果, 正確上傳格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202129,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startTime=0\n",
    "endTIme=0\n",
    "search_start=True\n",
    "search_end=False\n",
    "abnormal_list=[]\n",
    "shift = test_timeidx_new.index[0]\n",
    "for i,v in enumerate(pred_T):\n",
    "    i=i+shift\n",
    "    if (v==1) & (search_start):\n",
    "        startTime = test_timeidx_new[i]\n",
    "        search_end=True\n",
    "        search_start=False\n",
    "    if (v==0) & (search_end):\n",
    "        endTIme=test_timeidx_new[i]\n",
    "        search_start=True\n",
    "        search_end=False\n",
    "        abnormal_list.append((startTime,test_timeidx_new[i-1]))\n",
    "len(abnormal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "output_ans=True\n",
    "if(output_ans):\n",
    "    with open('test1_08_results.csv','w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['startTime','endTime'])\n",
    "        for row in abnormal_list:\n",
    "            dummy = csv_out.writerow(row)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1122.0, 1123.0), (1125.0, 1193.0), (1302.0, 1304.0), (1332.0, 1332.0)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnormal_list[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Issue\n",
    "1. Data Imbalance\n",
    " - training batch sample 一半normal一半abnormal的data\n",
    " - 用兩台風機訓練兩個model, 取故障預測的聯集 \n",
    "\n",
    "2. 用Precision & Recall評估好壞, 算出Score\n",
    "\n",
    "\n",
    "other:\n",
    "- using weighted examples. Just amplify the per-instance loss by a larger weight when seeing positive examples. If you use online gradient descent, it can be as simple as using a larger learning rate when seeing positive examples.\n",
    "- A similar and slightly better approach (only if you use stochastic gradient descent) is randomly picking an example in each iteration, where the positive examples have higher probability of being picked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "提交结果历史记录  \n",
    "参赛队伍:　556 / 参赛人数:　858  \n",
    " \n",
    "|竞赛阶段\t |上传者\t |分数\t |提交日期\t |排名\t |下载|\n",
    "| ---------| -------- | ------ | ------ | ------ | ------ |\n",
    "|初赛test1阶段|\t孔祥千\t|55.28684214|\t2017/7/12|\t50\t|下载|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 7/13\n",
    "1. cross correlation\n",
    "2. auto correlation\n",
    "3. common filter\n",
    "4. de-train "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
